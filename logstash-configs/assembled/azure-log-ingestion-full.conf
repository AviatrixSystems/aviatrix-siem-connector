# Aviatrix Log Integration Engine - Assembled Configuration
# Output Type: azure-log-ingestion
# Generated: 2026-02-13 18:23:49 UTC
#
# This file was automatically generated by assemble-config.sh
# Do not edit directly - modify the source modules instead:
#   - logstash-configs/inputs/
#   - logstash-configs/filters/
#   - logstash-configs/outputs/azure-log-ingestion/
#
# To regenerate: ./scripts/assemble-config.sh azure-log-ingestion

# ============================================================================
# INPUT CONFIGURATION
# ============================================================================

# Syslog Input Configuration
# Receives Aviatrix syslog messages on UDP/TCP port 5000

input {
    udp {
        port => 5000
        type => syslog
    }
    tcp {
        port => 5000
        type => syslog
    }
}

# ============================================================================
# FILTER CONFIGURATION
# ============================================================================

# FQDN Firewall Rule Filter
# Parses AviatrixFQDNRule syslog messages

filter {
    if [type] == "syslog" {
        grok {
            id => "fqdn"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["fqdn"]
            break_on_match => true
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*"
                ]
            }
        }
    }
}

# Controller CMD/API Filter
# Parses AviatrixCMD (V1) and AviatrixAPI (V2.5) syslog messages

# V1 API format (AviatrixCMD)
filter {
    if [type] == "syslog" and !("fqdn" in [tags]) {
        grok {
            id => "cmd-v1"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V1Api"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    # Format with username at end (reason may be empty)
                    "%{SYSLOG_TIMESTAMP:date}.*Controller-%{IP:controller_ip}.*AviatrixCMD: action=%{WORD:action}, argv=%{DATA:args}, result=%{WORD:result}, reason=%{DATA:reason}, username=%{NOTSPACE:username}",
                    # Format without username (reason may be empty)
                    "%{SYSLOG_TIMESTAMP:date}.*Controller-%{IP:controller_ip}.*AviatrixCMD: action=%{WORD:action}, argv=%{DATA:args}, result=%{WORD:result}, reason=%{DATA:reason}$"
                ]
            }
        }
    }
}

# Set gw_hostname for CMD logs (use controller_ip if available)
filter {
    if "cmd" in [tags] and [controller_ip] {
        mutate {
            id => "cmd-set-hostname"
            add_field => { "gw_hostname" => "Controller-%{controller_ip}" }
        }
    }
}

# Set default value for empty reason field in CMD logs
filter {
    if "cmd" in [tags] and (![reason] or [reason] == "") {
        mutate {
            id => "cmd-default-reason"
            replace => { "reason" => "" }
        }
    }
}

# V2.5 API format (AviatrixAPI)
filter {
    if [type] == "syslog" and !("fqdn" in [tags]) and !("cmd" in [tags]) {
        grok {
            id => "cmd-v2"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V2.5API"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixAPI.*url=%{GREEDYDATA:action} user=%{GREEDYDATA:username}? req_data=%{GREEDYDATA:args} resp_status=%{GREEDYDATA:result} resp_data=%{GREEDYDATA:reason}"
                ]
            }
        }
    }
}

# L4 Microsegmentation (eBPF) Filter
# Parses AviatrixGwMicrosegPacket syslog messages
# Supports both legacy 7.x and 8.2+ formats with session fields

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags]) {
        grok {
            id => "microseg"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["microseg", "ebpf"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    # 8.2+ format: GW-<name>-<ip>-sink with /usr/local/bin/avx-gw-state-sync process and session fields
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP:gw_ip}%{DATA}AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE:data_hex} ACT=%{WORD:action} ENFORCED=%{WORD:enforced} SESSION_ID=%{NUMBER:session_id} SESSION_EVENT=%{NUMBER:session_event} SESSION_END_REASON=%{NUMBER:session_end_reason} SESSION_PKT_CNT=%{NUMBER:session_pkt_cnt} SESSION_BYTE_CNT=%{NUMBER:session_byte_cnt} SESSION_DUR=%{NUMBER:session_dur}",

                    # 8.2+ format without session fields (first packet of flow)
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP:gw_ip}%{DATA}AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE:data_hex} ACT=%{WORD:action} ENFORCED=%{WORD:enforced}",

                    # Legacy format (SPT/DPT/ACTION)
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +%{HOSTNAME:gw_hostname} +AviatrixGwMicrosegPacket: SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} PROTO=%{WORD:proto} SPT=%{NUMBER:src_port} DPT=%{NUMBER:dst_port} ACTION=%{WORD:action}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }
    }
}

# Set default values for fields missing in legacy microseg format
filter {
    if "microseg" in [tags] and ![src_ip] {
        mutate {
            id => "microseg-legacy-defaults"
            add_field => {
                "src_ip" => "unknown"
                "dst_ip" => "unknown"
                "uuid" => "legacy-format"
                "enforced" => "unknown"
            }
        }
    }
}

# L7 DCF / MITM (TLS Inspection) Filter
# Parses traffic_server JSON syslog messages

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags]) {
        grok {
            id => "mitm"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["mitm"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    # traffic_server JSON format with syslog priority
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP}%{DATA}traffic_server\[%{NUMBER}\]: %{GREEDYDATA:[@metadata][json_payload]}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }

        # Parse JSON payload
        if "mitm" in [tags] {
            json {
                id => "mitm-json"
                skip_on_invalid_json => true
                source => "[@metadata][json_payload]"
                target => "[@metadata][payload]"
            }
        }

        # Convert MITM to microseg format
        if "mitm" in [tags] and [@metadata][payload] and "_jsonparsefailure" not in [tags] {
            # Use timestamp provided by MITM instead of syslog timestamp
            date {
                id => "mitm-timestamp"
                match => [ "[@metadata][payload][timestamp]", "UNIX" ]
                target => "@timestamp"
                remove_field => "date"
            }

            # Map MITM fields common to all traffic_server events
            mutate {
                id => "mitm-map-to-microseg"
                add_field => {
                    "proto" => "TCP"
                    "action" => "%{[@metadata][payload][action]}"
                    "src_ip" => "%{[@metadata][payload][src]}"
                    "src_port" => "%{[@metadata][payload][src_port]}"
                    "dst_ip" => "%{[@metadata][payload][dest]}"
                    "dst_port" => "%{[@metadata][payload][dest_port]}"
                    "enforced" => "%{[@metadata][payload][enforced]}"
                    "reason" => "%{[@metadata][payload][reason]}"
                    "log_priority" => "%{[@metadata][payload][priority]}"
                    "ids" => "%{[@metadata][payload][ids]}"
                    "session_id" => "%{[@metadata][payload][session_id]}"
                    "session_stage" => "%{[@metadata][payload][session_stage]}"
                    "session_start" => "%{[@metadata][payload][session_start]}"
                }
            }

            # Normalize DROP action to DENY
            if [@metadata][payload][action] == "DROP" {
                mutate {
                    id => "mitm-map-drop-to-deny"
                    replace => {
                        "[@metadata][payload][action]" => "DENY"
                    }
                }
            }

            # Add decided_by as uuid if present (absent in CONNECTION_SYN_ONLY_TIMEOUT)
            if [@metadata][payload][decided_by] {
                mutate {
                    id => "mitm-decided-by"
                    add_field => {
                        "uuid" => "%{[@metadata][payload][decided_by]}"
                    }
                }
            }

            # Add SNI hostname if present (absent in CONNECTION_SYN_ONLY_TIMEOUT)
            if [@metadata][payload][sni_hostname] {
                mutate {
                    id => "mitm-sni-hostname"
                    add_field => {
                        "mitm_sni_hostname" => "%{[@metadata][payload][sni_hostname]}"
                    }
                }
            }

            # Add URL if present
            if [@metadata][payload][url] {
                mutate {
                    id => "mitm-url-parts"
                    add_field => {
                        "mitm_url_parts" => "%{[@metadata][payload][url]}"
                    }
                }
            }

            # Add decrypted_by if present
            if [@metadata][payload][decrypted_by] {
                mutate {
                    id => "mitm-decrypted-by"
                    add_field => {
                        "mitm_decrypted_by" => "%{[@metadata][payload][decrypted_by]}"
                    }
                }
            }

            # Add session_time if present (absent in session_start events)
            if [@metadata][payload][session_time] {
                mutate {
                    id => "mitm-session-time"
                    add_field => {
                        "session_time" => "%{[@metadata][payload][session_time]}"
                    }
                }
            }

            # Add byte counters if present (only in session end events)
            if [@metadata][payload][request_bytes] {
                mutate {
                    id => "mitm-request-bytes"
                    add_field => {
                        "request_bytes" => "%{[@metadata][payload][request_bytes]}"
                    }
                }
            }

            if [@metadata][payload][response_bytes] {
                mutate {
                    id => "mitm-response-bytes"
                    add_field => {
                        "response_bytes" => "%{[@metadata][payload][response_bytes]}"
                    }
                }
            }

            # Add Suricata signature ID if present (only in IPS/IDS events)
            if [@metadata][payload][sid] {
                mutate {
                    id => "mitm-sid"
                    add_field => {
                        "sid" => "%{[@metadata][payload][sid]}"
                    }
                }
            }

        }
    }
}

# Suricata IDS/IPS Filter
# Parses Suricata JSON syslog messages from Aviatrix gateways
#
# Behavior controlled by FLATTEN_SURICATA environment variable:
#   FLATTEN_SURICATA=true  → Flattens nested objects for Splunk (recommended)
#   FLATTEN_SURICATA unset → Keeps nested JSON structure for Azure/other outputs
#
# When flattening is enabled:
#   alert.*    → top level (signature, severity, category, etc.)
#   flow.*     → flow_* (pkts_toserver, bytes_toclient, etc.)
#   http.*     → http_* (hostname, url, method, status, user_agent)
#   tls.*      → tls_* (sni, subject, issuer, ja3, fingerprint)
#   dns.*      → dns_* (query, rrname, rrtype, rdata, ttl)
#   smtp.*     → smtp_* (mail_from, rcpt_to, subject)
#   ssh.*      → ssh_client_*, ssh_server_* (software_version, hassh)
#   fileinfo.* → file_* (filename, size, md5, sha256)
#   tcp.*      → tcp_* (state, tcp_flags)
#   metadata.* → meta_* (confidence, signature_severity - arrays take first value)
#
# Always drops high-volume/low-value fields: files, payload, payload_printable, packet

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags]) {
        grok {
            id => "suricata"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["suricata"]
            match => {
                "message" => [
                    # Suricata JSON alert format with syslog priority
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP}%{DATA}suricata\[%{NUMBER}\]: %{GREEDYDATA:suricataData}"
                ]
            }
        }

        # Parse JSON data if it looks like valid JSON (starts with { and contains "event_type")
        if "suricata" in [tags] and [suricataData] =~ /^\{.*"event_type"/ {
            json {
                id => "suricata-data"
                skip_on_invalid_json => true
                source => "suricataData"
                target => "suricataDataJson"
            }
        }

        # Drop non-JSON suricata logs (notices, startup messages)
        if "suricata" in [tags] and !([suricataData] =~ /^\{/) {
            drop {
                id => "suricata-non-json-drop"
            }
        }

        # Drop events that failed JSON parsing
        if "_jsonparsefailure" in [tags] {
            drop {
                id => "suricata-json-failure-drop"
            }
        }

        # Drop Suricata stats events (too verbose, not security relevant)
        if [suricataDataJson][event_type] == "stats" {
            drop {
                id => "suricata-stats-drop"
            }
        }

        # Process Suricata JSON data
        # When FLATTEN_SURICATA=true: Flattens all nested objects for Splunk
        # When FLATTEN_SURICATA!=true: Keeps nested structure for Azure/other outputs
        if "suricata" in [tags] and [suricataDataJson] {
            ruby {
                id => "suricata-process"
                code => '
                    require "json"
                    require "time"

                    data = event.get("suricataDataJson")
                    next unless data.is_a?(Hash)

                    # Fields to drop (high-volume/low-value) - applies to all outputs
                    drop_fields = ["files", "payload", "payload_printable", "packet", "tx_guessed", "policy_id"]
                    tls_skip = ["certificate", "chain"]

                    # Calculate unix timestamp from Suricata timestamp
                    unix_time = nil
                    if data["timestamp"]
                        begin
                            unix_time = Time.parse(data["timestamp"]).to_i
                        rescue
                            unix_time = Time.now.to_i
                        end
                    else
                        unix_time = Time.now.to_i
                    end

                    # Check if flattening is enabled (for Splunk)
                    if ENV["FLATTEN_SURICATA"] == "true"
                        # Helper to flatten a nested object with a prefix
                        def flatten_object(data, obj_name, prefix, skip_keys = [])
                            return {} unless data[obj_name].is_a?(Hash)
                            result = {}
                            data[obj_name].each do |k, v|
                                next if skip_keys.include?(k)
                                if v.is_a?(Hash)
                                    # Handle double-nested objects (e.g., ssh.client.software_version)
                                    v.each do |k2, v2|
                                        next if v2.is_a?(Hash) || v2.is_a?(Array)
                                        result["#{prefix}_#{k}_#{k2}"] = v2
                                    end
                                elsif v.is_a?(Array)
                                    # Take first element for arrays (if primitive)
                                    result["#{prefix}_#{k}"] = v[0] if v[0] && !v[0].is_a?(Hash) && !v[0].is_a?(Array)
                                else
                                    result["#{prefix}_#{k}"] = v
                                end
                            end
                            result
                        end

                        flattened = {}

                        # Nested objects to flatten with their prefixes
                        nested_objects = {
                            "flow" => "flow",
                            "http" => "http",
                            "tls" => "tls",
                            "dns" => "dns",
                            "smtp" => "smtp",
                            "ssh" => "ssh",
                            "fileinfo" => "file",
                            "tcp" => "tcp"
                        }

                        # Process each top-level field
                        data.each do |key, value|
                            next if drop_fields.include?(key)

                            if key == "alert" && value.is_a?(Hash)
                                # Flatten alert fields to top level (no prefix)
                                value.each do |alert_key, alert_value|
                                    next if alert_value.is_a?(Hash) || alert_value.is_a?(Array)
                                    flattened[alert_key] = alert_value
                                end
                            elsif key == "metadata" && value.is_a?(Hash)
                                # Handle metadata specially - arrays take first value, skip timestamps
                                value.each do |meta_key, meta_value|
                                    next if ["created_at", "updated_at"].include?(meta_key)
                                    if meta_value.is_a?(Array)
                                        flattened["meta_#{meta_key}"] = meta_value[0] if meta_value[0]
                                    elsif !meta_value.is_a?(Hash)
                                        flattened["meta_#{meta_key}"] = meta_value
                                    end
                                end
                            elsif nested_objects.key?(key)
                                # Flatten known nested objects with appropriate prefix
                                skip_keys = (key == "tls") ? tls_skip : []
                                flattened.merge!(flatten_object(data, key, nested_objects[key], skip_keys))
                            elsif !value.is_a?(Hash) && !value.is_a?(Array)
                                # Keep primitive top-level fields as-is
                                flattened[key] = value
                            end
                        end

                        # Fix http_ prefix duplication (http.http_method → http_method not http_http_method)
                        flattened.keys.select { |k| k.start_with?("http_http_") }.each do |k|
                            new_key = k.sub("http_http_", "http_")
                            flattened[new_key] = flattened.delete(k)
                        end

                        event_data = flattened
                    else
                        # Non-flattened mode: keep nested structure but remove unwanted fields
                        cleaned = data.dup
                        drop_fields.each { |f| cleaned.delete(f) }
                        if cleaned["tls"].is_a?(Hash)
                            tls_skip.each { |f| cleaned["tls"].delete(f) }
                        end
                        event_data = cleaned
                    end

                    # Build complete HEC payload with event as JSON object
                    payload = {
                        "sourcetype" => "aviatrix:ids",
                        "source" => "avx-ids",
                        "host" => event.get("gw_hostname"),
                        "time" => unix_time,
                        "event" => event_data
                    }
                    event.set("[@metadata][suricata_hec_payload]", payload.to_json)
                '
            }
        }
    }
}

# Gateway Performance Statistics Filter
# Parses AviatrixGwNetStats and AviatrixGwSysStats syslog messages

# Network statistics (interface throughput)
filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags] or "suricata" in [tags]) {
        grok {
            id => "gw_net_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_net_stats"]
            break_on_match => true
            match => {
                "message" => [
                    # With syslog priority prefix and all optional fields
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date}%{DATA}AviatrixGwNetStats:%{DATA}name=%{NOTSPACE:gateway}(?:%{DATA}alias=%{NOTSPACE:alias})?%{DATA}(?:public_ip=%{IP:public_ip}%{DATA})?private_ip=%{IP:private_ip}%{DATA}interface=%{NOTSPACE:interface}%{DATA}total_rx_rate=%{NOTSPACE:total_rx_rate}%{DATA}total_tx_rate=%{NOTSPACE:total_tx_rate}%{DATA}total_rx_tx_rate=%{NOTSPACE:total_rx_tx_rate}%{DATA}total_rx_cum=%{NOTSPACE:total_rx_cum}%{DATA}total_tx_cum=%{NOTSPACE:total_tx_cum}%{DATA}total_rx_tx_cum=%{NOTSPACE:total_rx_tx_cum}(?:%{DATA}conntrack_limit_exceeded=%{NUMBER:conntrack_limit_exceeded})?(?:%{DATA}bw_in_limit_exceeded=%{NUMBER:bw_in_limit_exceeded})?(?:%{DATA}bw_out_limit_exceeded=%{NUMBER:bw_out_limit_exceeded})?(?:%{DATA}pps_limit_exceeded=%{NUMBER:pps_limit_exceeded})?(?:%{DATA}linklocal_limit_exceeded=%{NUMBER:linklocal_limit_exceeded})?(?:%{DATA}conntrack_count=%{NUMBER:conntrack_count})?(?:%{DATA}conntrack_allowance_available=%{NUMBER:conntrack_allowance_available})?(?:%{DATA}conntrack_usage_rate=%{NUMBER:conntrack_usage_rate})?"
                ]
            }
        }
    }
}

# System statistics (CPU, memory, disk)
filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags] or "suricata" in [tags] or "gw_net_stats" in [tags]) {
        grok {
            id => "gw_sys_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_sys_stats"]
            break_on_match => true
            match => {
                "message" => [
                    # With optional alias and cpu_cores fields
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date}%{DATA}AviatrixGwSysStats:%{DATA}name=%{NOTSPACE:gateway}(?:%{DATA}alias=%{NOTSPACE:alias})?%{DATA}cpu_idle=%{NUMBER:cpu_idle}%{DATA}memory_free=%{NUMBER:memory_free}%{DATA}memory_available=%{NUMBER:memory_available}%{DATA}memory_total=%{NUMBER:memory_total}%{DATA}disk_total=%{NUMBER:disk_total}%{DATA}disk_free=%{NUMBER:disk_free}(?:%{DATA}cpu_cores=%{GREEDYDATA:cpu_cores})?"
                ]
            }
        }
    }
}

# Parse cpu_cores into structured per-core metrics
filter {
    if "gw_sys_stats" in [tags] and [cpu_cores] {
        ruby {
            id => "gw_sys_stats-parse-cpu-cores"
            code => '
                cpu_cores_str = event.get("cpu_cores")
                return unless cpu_cores_str

                # Array to hold all core metrics
                cores = []

                # Match each core block: name:X busy:{...}
                # Handles both "name:X busy:{...}" and standalone "busy:{...}" entries
                core_blocks = cpu_cores_str.scan(/(?:name:([-\d]+)\s+)?busy:\{([^}]+)\}/)

                core_blocks.each_with_index do |match, idx|
                    core_name = match[0] || "unknown_#{idx}"
                    busy_data = match[1]

                    core = {
                        "name" => core_name,
                        "core_id" => core_name
                    }

                    # Extract min, max, avg from busy data
                    if busy_data =~ /min:(\d+)/
                        core["busy_min"] = $1.to_i
                    end
                    if busy_data =~ /max:(\d+)/
                        core["busy_max"] = $1.to_i
                    end
                    if busy_data =~ /avg:(\d+)/
                        core["busy_avg"] = $1.to_i
                    end

                    # Extract timestamps if needed (usually not necessary for alerting)
                    # but keeping for completeness
                    if busy_data =~ /start:\{seconds:(\d+)\s+nanos:(\d+)\}/
                        core["start_seconds"] = $1.to_i
                        core["start_nanos"] = $2.to_i
                    end
                    if busy_data =~ /end:\{seconds:(\d+)\s+nanos:(\d+)\}/
                        core["end_seconds"] = $1.to_i
                        core["end_nanos"] = $2.to_i
                    end

                    cores << core
                end

                # Store parsed cores as an array
                event.set("cpu_cores_parsed", cores)

                # Also extract aggregate (-1) core metrics to top level for easy access
                aggregate = cores.find { |c| c["name"] == "-1" }
                if aggregate
                    event.set("cpu_aggregate_busy_min", aggregate["busy_min"]) if aggregate["busy_min"]
                    event.set("cpu_aggregate_busy_max", aggregate["busy_max"]) if aggregate["busy_max"]
                    event.set("cpu_aggregate_busy_avg", aggregate["busy_avg"]) if aggregate["busy_avg"]
                end
            '
        }
    }
}

# Tunnel Status Change Filter
# Parses AviatrixTunnelStatusChange syslog messages

filter {
    if [type] == "syslog" {
        grok {
            id => "tunnel_status"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["tunnel_status"]
            break_on_match => true
            match => {
                "message" => [
                    "^%{SYSLOG_TIMESTAMP:date}.*AviatrixTunnelStatusChange.*src_gw=%{TUNNEL_GW:src_gw}.*dst_gw=%{TUNNEL_GW:dst_gw}.*old_state=%{WORD:old_state}.*new_state=%{WORD:new_state}.*"
                ]
            }
        }
    }
}

# Microseg Throttling Filter
# Limits log volume for L4 microseg events to max 2 logs/minute per connection
# This reduces storage costs while maintaining visibility

filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Throttle key includes policy UUID, IPs, ports, and protocol
        # This ensures unique connections are tracked independently
        # Note: Uses dst_ip (correct field name from grok pattern)
        throttle {
            id => "microseg-throttle"
            key => "%{uuid}%{src_ip}%{dst_ip}%{src_port}%{dst_port}%{proto}"
            max_age => 120
            period => "60"
            after_count => 1
            add_tag => "throttled"
        }
    }
}

# Drop throttled events
filter {
    if "throttled" in [tags] {
        drop {
            id => "microseg-throttled"
        }
    }
}

# Timestamp Normalization Filter
# Converts the parsed date field to @timestamp and adds unix_time

filter {
    date {
        id => "date-to-timestamp"
        # Supports multiple timestamp formats:
        # - ISO8601: 2022-05-14T03:46:10.257442+00:00
        # - Long format: 2022-05-14 03:46:10.257442
        # - Short format with double space: Dec  9 03:46:16
        # - Short format with single space: Dec 14 03:46:16
        match => [ "date", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSSSSS", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        target => "@timestamp"
        remove_field => [ "date" ]
    }

    # Add unix timestamp for outputs that require epoch time
    ruby {
        id => "add-unix-time"
        code => "event.set('unix_time', event.get('@timestamp').to_i)"
    }
}

# Field Type Conversion Filter
# Converts string fields to their proper types for downstream processing

# Microseg field conversions
filter {
    if "microseg" in [tags] {
        mutate {
            id => "microseg-field-conversion"
            convert => {
                "src_port" => "integer"
                "dst_port" => "integer"
                "enforced" => "boolean"
            }
        }
    }
}

# Gateway network stats field conversions
filter {
    if "gw_net_stats" in [tags] {
        mutate {
            id => "gw_net_stats-rate-conversion"
            convert => {
                "total_rx_rate" => "integer"
                "total_tx_rate" => "integer"
                "total_rx_tx_rate" => "integer"
            }
            gsub => [
                "total_rx_rate", "Kb", "000",
                "total_tx_rate", "Kb", "000",
                "total_rx_tx_rate", "Kb", "000"
            ]
        }
    }
}

# Gateway system stats field conversions
filter {
    if "gw_sys_stats" in [tags] {
        mutate {
            id => "gw_sys_stats-field-conversion"
            convert => {
                "cpu_idle" => "float"
                "memory_free" => "integer"
                "memory_available" => "integer"
                "memory_total" => "integer"
                "disk_total" => "integer"
                "disk_free" => "integer"
            }
        }
    }
}

# ============================================================================
# OUTPUT CONFIGURATION (azure-log-ingestion)
# ============================================================================

# Azure Log Analytics / Sentinel Output (via Data Collection Rules)
# Uses the Microsoft Sentinel Log Analytics Logstash output plugin
#
# Supported Log Types:
#   - Suricata IDS alerts → Custom-AviatrixSuricata_CL
#   - L7 MITM/DCF (TLS inspection) → Custom-AviatrixMicroseg_CL
#   - L4 Microsegmentation → Custom-AviatrixMicroseg_CL
#
# Environment Variables:
#   client_app_id           - Azure AD application (service principal) ID
#   client_app_secret       - Azure AD application secret
#   tenant_id               - Azure AD tenant ID
#   data_collection_endpoint - Data Collection Endpoint URL
#   azure_dcr_suricata_id   - DCR immutable ID for Suricata logs
#   azure_stream_suricata   - Stream name for Suricata logs
#   azure_dcr_microseg_id   - DCR immutable ID for Microseg/MITM logs
#   azure_stream_microseg   - Stream name for Microseg/MITM logs
#   azure_cloud             - "AzureCloud", "AzureChinaCloud", or "AzureUSGovernment"

# Suricata-specific pre-processing for Azure
filter {
    if "suricata" in [tags] {
        mutate {
            id => "suricata-azure-cleanup"
            remove_field => ["message", "suricataData", "gw_hostname", "host", "port", "type", "event"]
        }

        # Add TimeGenerated field and flatten suricataDataJson for Azure
        ruby {
            id => "suricata-azure-flatten"
            code => "
                if event.get('suricataDataJson')
                    # Add TimeGenerated field required by Azure
                    event.set('TimeGenerated', event.get('@timestamp'))

                    # Flatten suricataDataJson into the event root
                    event.get('suricataDataJson').each { |k, v| event.set(k, v) }
                    event.remove('suricataDataJson')
                end
            "
        }
    }
}

# Microseg pre-processing for Azure
filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Add TimeGenerated field for Azure
        ruby {
            id => "microseg-azure-timegen"
            code => "event.set('TimeGenerated', event.get('@timestamp'))"
        }
    }
}

# MITM/L7 DCF pre-processing for Azure
filter {
    if "mitm" in [tags] {
        # Add TimeGenerated field for Azure
        ruby {
            id => "mitm-azure-timegen"
            code => "event.set('TimeGenerated', event.get('@timestamp'))"
        }
    }
}

output {
    # DEBUG: Output all events to stdout to verify processing
    stdout {
        id => "debug-stdout"
        codec => rubydebug {
            metadata => true
        }
    }

    # Suricata IDS events to Azure
    if "suricata" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-suricata"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_suricata_id}"
            dcr_stream_name => "${azure_stream_suricata}"
            azure_cloud => "${azure_cloud}"
        }
    }

    # L7 DCF / MITM events to Azure (TLS inspection)
    # MITM events are also tagged with "microseg" so they'll go to both outputs
    else if "mitm" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-mitm"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_microseg_id}"
            dcr_stream_name => "${azure_stream_microseg}"
            azure_cloud => "${azure_cloud}"
        }
    }

    # L4 Microseg events to Azure
    else if "microseg" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-microseg"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_microseg_id}"
            dcr_stream_name => "${azure_stream_microseg}"
            azure_cloud => "${azure_cloud}"
        }
    }
}
