# Aviatrix Log Integration Engine - Assembled Configuration
# Output Type: azure-log-ingestion
# Generated: 2026-02-14 14:39:18 UTC
#
# This file was automatically generated by assemble-config.sh
# Do not edit directly - modify the source modules instead:
#   - logstash-configs/inputs/
#   - logstash-configs/filters/
#   - logstash-configs/outputs/azure-log-ingestion/
#
# To regenerate: ./scripts/assemble-config.sh azure-log-ingestion

# ============================================================================
# INPUT CONFIGURATION
# ============================================================================

# Syslog Input Configuration
# Receives Aviatrix syslog messages on UDP/TCP port 5000

input {
    udp {
        port => 5000
        type => syslog
    }
    tcp {
        port => 5000
        type => syslog
    }
}

# ============================================================================
# FILTER CONFIGURATION
# ============================================================================

# FQDN Firewall Rule Filter
# Parses AviatrixFQDNRule syslog messages

filter {
    if [type] == "syslog" {
        grok {
            id => "fqdn"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["fqdn"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*"
                ]
            }
        }
    }
}

# Controller CMD/API Filter
# Parses AviatrixCMD (V1) and AviatrixAPI (V2.5) syslog messages

# V1 API format (AviatrixCMD)
filter {
    if [type] == "syslog" and !("fqdn" in [tags]) {
        grok {
            id => "cmd-v1"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V1Api"]
            tag_on_failure => []
            match => {
                "message" => [
                    # Format with username at end (reason may be empty)
                    "%{SYSLOG_TIMESTAMP:date}.*Controller-%{IP:controller_ip}.*AviatrixCMD: action=%{WORD:action}, argv=%{DATA:args}, result=%{WORD:result}, reason=%{DATA:reason}, username=%{NOTSPACE:username}",
                    # Format without username (reason may be empty)
                    "%{SYSLOG_TIMESTAMP:date}.*Controller-%{IP:controller_ip}.*AviatrixCMD: action=%{WORD:action}, argv=%{DATA:args}, result=%{WORD:result}, reason=%{DATA:reason}$"
                ]
            }
        }
    }
}

# Set gw_hostname for CMD logs (use controller_ip if available)
filter {
    if "cmd" in [tags] and [controller_ip] {
        mutate {
            id => "cmd-set-hostname"
            add_field => { "gw_hostname" => "Controller-%{controller_ip}" }
        }
    }
}

# Set default value for empty reason field in CMD logs
filter {
    if "cmd" in [tags] and (![reason] or [reason] == "") {
        mutate {
            id => "cmd-default-reason"
            replace => { "reason" => "" }
        }
    }
}

# V2.5 API format (AviatrixAPI)
filter {
    if [type] == "syslog" and !("fqdn" in [tags]) and !("cmd" in [tags]) {
        grok {
            id => "cmd-v2"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V2.5API"]
            tag_on_failure => []
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixAPI.*url=%{GREEDYDATA:action} user=%{GREEDYDATA:username}? req_data=%{GREEDYDATA:args} resp_status=%{GREEDYDATA:result} resp_data=%{GREEDYDATA:reason}"
                ]
            }
        }
    }
}

# L4 Microsegmentation (eBPF) Filter
# Parses AviatrixGwMicrosegPacket syslog messages
# Supports both legacy 7.x and 8.2+ formats with session fields

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags]) {
        grok {
            id => "microseg"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["microseg", "ebpf"]
            tag_on_failure => []
            match => {
                "message" => [
                    # 8.2+ format: GW-<name>-<ip>-sink with /usr/local/bin/avx-gw-state-sync process and session fields
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP:gw_ip}%{DATA}AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE:data_hex} ACT=%{WORD:action} ENFORCED=%{WORD:enforced} SESSION_ID=%{NUMBER:session_id} SESSION_EVENT=%{NUMBER:session_event} SESSION_END_REASON=%{NUMBER:session_end_reason} SESSION_PKT_CNT=%{NUMBER:session_pkt_cnt} SESSION_BYTE_CNT=%{NUMBER:session_byte_cnt} SESSION_DUR=%{NUMBER:session_dur}",

                    # 8.2+ format without session fields (first packet of flow)
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP:gw_ip}%{DATA}AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE:data_hex} ACT=%{WORD:action} ENFORCED=%{WORD:enforced}",

                    # Legacy format (SPT/DPT/ACTION)
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +%{HOSTNAME:gw_hostname} +AviatrixGwMicrosegPacket: SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} PROTO=%{WORD:proto} SPT=%{NUMBER:src_port} DPT=%{NUMBER:dst_port} ACTION=%{WORD:action}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }
    }
}

# Set default values for fields missing in legacy microseg format
filter {
    if "microseg" in [tags] and ![src_ip] {
        mutate {
            id => "microseg-legacy-defaults"
            add_field => {
                "src_ip" => "unknown"
                "dst_ip" => "unknown"
                "uuid" => "legacy-format"
                "enforced" => "unknown"
            }
        }
    }
}

# L7 DCF / MITM (TLS Inspection) Filter
# Parses traffic_server JSON syslog messages

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags]) {
        grok {
            id => "mitm"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["mitm"]
            tag_on_failure => []
            match => {
                "message" => [
                    # traffic_server JSON format with syslog priority
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP}%{DATA}traffic_server\[%{NUMBER}\]: %{GREEDYDATA:[@metadata][json_payload]}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }

        # Parse JSON payload
        if "mitm" in [tags] {
            json {
                id => "mitm-json"
                skip_on_invalid_json => true
                source => "[@metadata][json_payload]"
                target => "[@metadata][payload]"
            }
        }

        # Convert MITM to microseg format
        if "mitm" in [tags] and [@metadata][payload] and "_jsonparsefailure" not in [tags] {
            # Use timestamp provided by MITM instead of syslog timestamp
            date {
                id => "mitm-timestamp"
                match => [ "[@metadata][payload][timestamp]", "UNIX" ]
                target => "@timestamp"
                remove_field => "date"
            }

            # Map MITM fields to microseg fields
            mutate {
                id => "mitm-map-to-microseg"
                add_field => {
                    "proto" => "TCP"
                    "action" => "%{[@metadata][payload][action]}"
                    "src_ip" => "%{[@metadata][payload][src]}"
                    "src_port" => "%{[@metadata][payload][src_port]}"
                    "dst_ip" => "%{[@metadata][payload][dest]}"
                    "dst_port" => "%{[@metadata][payload][dest_port]}"
                    "enforced" => "%{[@metadata][payload][enforced]}"
                    "uuid" => "%{[@metadata][payload][decided_by]}"
                    "mitm_sni_hostname" => "%{[@metadata][payload][sni_hostname]}"
                }
            }

            # Normalize DROP action to DENY
            if [@metadata][payload][action] == "DROP" {
                mutate {
                    id => "mitm-map-drop-to-deny"
                    replace => {
                        "[@metadata][payload][action]" => "DENY"
                    }
                }
            }

            # Add URL if present
            if [@metadata][payload][url] {
                mutate {
                    id => "mitm-url-parts"
                    add_field => {
                        "mitm_url_parts" => "%{[@metadata][payload][url]}"
                    }
                }
            }

            # Add decrypted_by if present
            if [@metadata][payload][decrypted_by] {
                mutate {
                    id => "mitm-decrypted-by"
                    add_field => {
                        "mitm_decrypted_by" => "%{[@metadata][payload][decrypted_by]}"
                    }
                }
            }

        }
    }
}

# Suricata IDS/IPS Filter
# Parses Suricata JSON syslog messages from Aviatrix gateways
#
# Behavior controlled by FLATTEN_SURICATA environment variable:
#   FLATTEN_SURICATA=true  → Flattens nested objects for Splunk (recommended)
#   FLATTEN_SURICATA unset → Keeps nested JSON structure for Azure/other outputs
#
# When flattening is enabled:
#   alert.*    → top level (signature, severity, category, etc.)
#   flow.*     → flow_* (pkts_toserver, bytes_toclient, etc.)
#   http.*     → http_* (hostname, url, method, status, user_agent)
#   tls.*      → tls_* (sni, subject, issuer, ja3, fingerprint)
#   dns.*      → dns_* (query, rrname, rrtype, rdata, ttl)
#   smtp.*     → smtp_* (mail_from, rcpt_to, subject)
#   ssh.*      → ssh_client_*, ssh_server_* (software_version, hassh)
#   fileinfo.* → file_* (filename, size, md5, sha256)
#   tcp.*      → tcp_* (state, tcp_flags)
#   metadata.* → meta_* (confidence, signature_severity - arrays take first value)
#
# Always drops high-volume/low-value fields: files, payload, payload_printable, packet

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags]) {
        grok {
            id => "suricata"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["suricata"]
            tag_on_failure => []
            match => {
                "message" => [
                    # Suricata JSON alert format with syslog priority
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP}%{DATA}suricata\[%{NUMBER}\]: %{GREEDYDATA:suricataData}"
                ]
            }
        }

        # Parse JSON data if it looks like valid JSON (starts with { and contains "event_type")
        if "suricata" in [tags] and [suricataData] =~ /^\{.*"event_type"/ {
            json {
                id => "suricata-data"
                skip_on_invalid_json => true
                source => "suricataData"
                target => "suricataDataJson"
            }
        }

        # Drop non-JSON suricata logs (notices, startup messages)
        if "suricata" in [tags] and !([suricataData] =~ /^\{/) {
            drop {
                id => "suricata-non-json-drop"
            }
        }

        # Drop events that failed JSON parsing
        if "_jsonparsefailure" in [tags] {
            drop {
                id => "suricata-json-failure-drop"
            }
        }

        # Drop Suricata stats events (too verbose, not security relevant)
        if [suricataDataJson][event_type] == "stats" {
            drop {
                id => "suricata-stats-drop"
            }
        }

        # Process Suricata JSON data
        # When FLATTEN_SURICATA=true: Flattens all nested objects for Splunk
        # When FLATTEN_SURICATA!=true: Keeps nested structure for Azure/other outputs
        if "suricata" in [tags] and [suricataDataJson] {
            ruby {
                id => "suricata-process"
                code => '
                    require "json"
                    require "time"

                    data = event.get("suricataDataJson")
                    next unless data.is_a?(Hash)

                    # Fields to drop (high-volume/low-value) - applies to all outputs
                    drop_fields = ["files", "payload", "payload_printable", "packet", "tx_guessed", "policy_id"]
                    tls_skip = ["certificate", "chain"]

                    # Calculate unix timestamp from Suricata timestamp
                    unix_time = nil
                    if data["timestamp"]
                        begin
                            unix_time = Time.parse(data["timestamp"]).to_i
                        rescue
                            unix_time = Time.now.to_i
                        end
                    else
                        unix_time = Time.now.to_i
                    end

                    # Check if flattening is enabled (for Splunk)
                    if ENV["FLATTEN_SURICATA"] == "true"
                        # Helper to flatten a nested object with a prefix
                        def flatten_object(data, obj_name, prefix, skip_keys = [])
                            return {} unless data[obj_name].is_a?(Hash)
                            result = {}
                            data[obj_name].each do |k, v|
                                next if skip_keys.include?(k)
                                if v.is_a?(Hash)
                                    # Handle double-nested objects (e.g., ssh.client.software_version)
                                    v.each do |k2, v2|
                                        next if v2.is_a?(Hash) || v2.is_a?(Array)
                                        result["#{prefix}_#{k}_#{k2}"] = v2
                                    end
                                elsif v.is_a?(Array)
                                    # Take first element for arrays (if primitive)
                                    result["#{prefix}_#{k}"] = v[0] if v[0] && !v[0].is_a?(Hash) && !v[0].is_a?(Array)
                                else
                                    result["#{prefix}_#{k}"] = v
                                end
                            end
                            result
                        end

                        flattened = {}

                        # Nested objects to flatten with their prefixes
                        nested_objects = {
                            "flow" => "flow",
                            "http" => "http",
                            "tls" => "tls",
                            "dns" => "dns",
                            "smtp" => "smtp",
                            "ssh" => "ssh",
                            "fileinfo" => "file",
                            "tcp" => "tcp"
                        }

                        # Process each top-level field
                        data.each do |key, value|
                            next if drop_fields.include?(key)

                            if key == "alert" && value.is_a?(Hash)
                                # Flatten alert fields to top level (no prefix)
                                value.each do |alert_key, alert_value|
                                    next if alert_value.is_a?(Hash) || alert_value.is_a?(Array)
                                    flattened[alert_key] = alert_value
                                end
                            elsif key == "metadata" && value.is_a?(Hash)
                                # Handle metadata specially - arrays take first value, skip timestamps
                                value.each do |meta_key, meta_value|
                                    next if ["created_at", "updated_at"].include?(meta_key)
                                    if meta_value.is_a?(Array)
                                        flattened["meta_#{meta_key}"] = meta_value[0] if meta_value[0]
                                    elsif !meta_value.is_a?(Hash)
                                        flattened["meta_#{meta_key}"] = meta_value
                                    end
                                end
                            elsif nested_objects.key?(key)
                                # Flatten known nested objects with appropriate prefix
                                skip_keys = (key == "tls") ? tls_skip : []
                                flattened.merge!(flatten_object(data, key, nested_objects[key], skip_keys))
                            elsif !value.is_a?(Hash) && !value.is_a?(Array)
                                # Keep primitive top-level fields as-is
                                flattened[key] = value
                            end
                        end

                        # Fix http_ prefix duplication (http.http_method → http_method not http_http_method)
                        flattened.keys.select { |k| k.start_with?("http_http_") }.each do |k|
                            new_key = k.sub("http_http_", "http_")
                            flattened[new_key] = flattened.delete(k)
                        end

                        event_data = flattened
                    else
                        # Non-flattened mode: keep nested structure but remove unwanted fields
                        cleaned = data.dup
                        drop_fields.each { |f| cleaned.delete(f) }
                        if cleaned["tls"].is_a?(Hash)
                            tls_skip.each { |f| cleaned["tls"].delete(f) }
                        end
                        event_data = cleaned
                    end

                    # Build complete HEC payload with event as JSON object
                    payload = {
                        "sourcetype" => "aviatrix:ids",
                        "source" => "avx-ids",
                        "host" => event.get("gw_hostname"),
                        "time" => unix_time,
                        "event" => event_data
                    }
                    event.set("[@metadata][suricata_hec_payload]", payload.to_json)
                '
            }
        }
    }
}

# Gateway Performance Statistics Filter
# Parses AviatrixGwNetStats and AviatrixGwSysStats syslog messages

# Network statistics (interface throughput)
filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags] or "suricata" in [tags]) {
        grok {
            id => "gw_net_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_net_stats"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    # With syslog priority prefix and public_ip
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date}%{DATA}AviatrixGwNetStats:%{DATA}name=%{NOTSPACE:gateway}(?:%{DATA}alias=%{NOTSPACE:alias})?(?:%{DATA}public_ip=%{IP:public_ip})?%{DATA}private_ip=%{IP:private_ip}%{DATA}interface=%{NOTSPACE:interface}%{DATA}total_rx_rate=%{NOTSPACE:total_rx_rate}%{DATA}total_tx_rate=%{NOTSPACE:total_tx_rate}%{DATA}total_rx_tx_rate=%{NOTSPACE:total_rx_tx_rate}%{DATA}total_rx_cum=%{NOTSPACE:total_rx_cum}%{DATA}total_tx_cum=%{NOTSPACE:total_tx_cum}%{DATA}total_rx_tx_cum=%{NOTSPACE:total_rx_tx_cum}(?:%{DATA}conntrack_limit_exceeded=%{NUMBER:conntrack_limit_exceeded})?(?:%{DATA}bw_in_limit_exceeded=%{NUMBER:bw_in_limit_exceeded})?(?:%{DATA}bw_out_limit_exceeded=%{NUMBER:bw_out_limit_exceeded})?(?:%{DATA}pps_limit_exceeded=%{NUMBER:pps_limit_exceeded})?(?:%{DATA}linklocal_limit_exceeded=%{NUMBER:linklocal_limit_exceeded})?(?:%{DATA}conntrack_count=%{NUMBER:conntrack_count})?(?:%{DATA}conntrack_allowance_available=%{NUMBER:conntrack_allowance_available})?(?:%{DATA}conntrack_usage_rate=%{NUMBER:conntrack_usage_rate})?"
                ]
            }
        }
    }
}

# System statistics (CPU, memory, disk)
filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags] or "suricata" in [tags] or "gw_net_stats" in [tags]) {
        grok {
            id => "gw_sys_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_sys_stats"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date}%{DATA}AviatrixGwSysStats:%{DATA}name=%{NOTSPACE:gateway}(?:%{DATA}alias=%{NOTSPACE:alias})?%{DATA}cpu_idle=%{NUMBER:cpu_idle}%{DATA}memory_free=%{NUMBER:memory_free}%{DATA}memory_available=%{NUMBER:memory_available}%{DATA}memory_total=%{NUMBER:memory_total}%{DATA}disk_total=%{NUMBER:disk_total}%{DATA}disk_free=%{NUMBER:disk_free}(?:%{DATA}cpu_cores=%{GREEDYDATA:cpu_cores})?"
                ]
            }
        }
    }
}

# Tunnel Status Change Filter
# Parses AviatrixTunnelStatusChange syslog messages

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags] or "mitm" in [tags] or "suricata" in [tags] or "gw_net_stats" in [tags] or "gw_sys_stats" in [tags]) {
        grok {
            id => "tunnel_status"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["tunnel_status"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    "^%{SYSLOG_TIMESTAMP:date}.*AviatrixTunnelStatusChange.*src_gw=%{TUNNEL_GW:src_gw}.*dst_gw=%{TUNNEL_GW:dst_gw}.*old_state=%{WORD:old_state}.*new_state=%{WORD:new_state}.*"
                ]
            }
        }
    }
}

# CPU Cores Parser & System Stats HEC Payload Builder
# Parses protobuf-text cpu_cores field from AviatrixGwSysStats into structured JSON
# Also builds a Splunk HEC payload for gw_sys_stats events (same pattern as suricata)

# Step 1: Parse cpu_cores protobuf text into structured fields
filter {
    if "gw_sys_stats" in [tags] and [cpu_cores] and [cpu_cores] != "" {
        ruby {
            id => "cpu-cores-parse"
            code => '
                raw = event.get("cpu_cores")
                next unless raw.is_a?(String) && raw.length > 2

                # Strip outer brackets
                raw = raw.strip
                raw = raw[1..-2] if raw.start_with?("[") && raw.end_with?("]")

                # Tokenize into name:N and busy:{...} tokens with nested brace matching
                tokens = []
                i = 0
                while i < raw.length
                    # Skip whitespace
                    if raw[i] =~ /\s/
                        i += 1
                        next
                    end

                    # Match name:N token
                    if raw[i..] =~ /\Aname:(-?\d+)/
                        tokens << { type: :name, value: $1 }
                        i += $~[0].length
                        next
                    end

                    # Match busy:{...} token with nested brace matching
                    if raw[i..] =~ /\Abusy:\{/
                        depth = 0
                        start = i + 5  # skip "busy:"
                        j = start
                        while j < raw.length
                            if raw[j] == "{"
                                depth += 1
                            elsif raw[j] == "}"
                                depth -= 1
                                if depth == 0
                                    body = raw[start..j]
                                    tokens << { type: :busy, body: body }
                                    i = j + 1
                                    break
                                end
                            end
                            j += 1
                        end
                        # If we never closed braces, skip past what we matched
                        i = j + 1 if depth != 0
                        next
                    end

                    # Skip any unrecognized character
                    i += 1
                end

                next if tokens.empty?

                # Extract min/max/avg from a busy block body
                # Strips out start:{...} and end:{...} sub-blocks first
                extract_stats = lambda do |body|
                    # Remove nested start:{...} and end:{...} blocks
                    cleaned = body.gsub(/(?:start|end):\{[^}]*\}/, "")
                    stats = {}
                    cleaned.scan(/(min|max|avg):(\d+)/).each do |key, val|
                        stats[key] = val.to_i
                    end
                    stats
                end

                # Detect format: name-first vs busy-first
                name_first = tokens[0][:type] == :name

                # Pair tokens into core entries
                cores = []
                if name_first
                    pending_name = nil
                    tokens.each do |tok|
                        if tok[:type] == :name
                            pending_name = tok[:value]
                        elsif tok[:type] == :busy
                            stats = extract_stats.call(tok[:body])
                            entry = {}
                            entry["name"] = pending_name if pending_name
                            entry["busy_min"] = stats["min"] if stats["min"]
                            entry["busy_max"] = stats["max"] if stats["max"]
                            entry["busy_avg"] = stats["avg"] if stats["avg"]
                            cores << entry
                            pending_name = nil
                        end
                    end
                else
                    # busy-first: each busy optionally consumes the following name
                    idx = 0
                    while idx < tokens.length
                        tok = tokens[idx]
                        if tok[:type] == :busy
                            stats = extract_stats.call(tok[:body])
                            entry = {}
                            # Check if next token is a name
                            if idx + 1 < tokens.length && tokens[idx + 1][:type] == :name
                                entry["name"] = tokens[idx + 1][:value]
                                idx += 1
                            end
                            entry["busy_min"] = stats["min"] if stats["min"]
                            entry["busy_max"] = stats["max"] if stats["max"]
                            entry["busy_avg"] = stats["avg"] if stats["avg"]
                            cores << entry
                        elsif tok[:type] == :name
                            # Standalone name followed by busy
                            if idx + 1 < tokens.length && tokens[idx + 1][:type] == :busy
                                stats = extract_stats.call(tokens[idx + 1][:body])
                                entry = { "name" => tok[:value] }
                                entry["busy_min"] = stats["min"] if stats["min"]
                                entry["busy_max"] = stats["max"] if stats["max"]
                                entry["busy_avg"] = stats["avg"] if stats["avg"]
                                cores << entry
                                idx += 1
                            end
                        end
                        idx += 1
                    end
                end

                next if cores.empty?

                # Set parsed array
                event.set("cpu_cores_parsed", cores)

                # Extract aggregate (name=-1) and individual core stats
                aggregate = nil
                individual_count = 0
                cores.each do |c|
                    if c["name"] == "-1"
                        aggregate = c
                    elsif c["name"] && c["name"] != "-1"
                        individual_count += 1
                    end
                end

                if aggregate
                    event.set("cpu_aggregate_busy_min", aggregate["busy_min"]) if aggregate["busy_min"]
                    event.set("cpu_aggregate_busy_max", aggregate["busy_max"]) if aggregate["busy_max"]
                    event.set("cpu_aggregate_busy_avg", aggregate["busy_avg"]) if aggregate["busy_avg"]
                    # cpu_busy: prefer avg, fall back to max
                    if aggregate["busy_avg"]
                        event.set("cpu_busy", aggregate["busy_avg"])
                    elsif aggregate["busy_max"]
                        event.set("cpu_busy", aggregate["busy_max"])
                    end
                end

                # Fallback: compute cpu_busy from cpu_idle if no aggregate found
                if !aggregate
                    cpu_idle = event.get("cpu_idle")
                    if cpu_idle
                        idle_val = cpu_idle.is_a?(Numeric) ? cpu_idle : cpu_idle.to_f
                        event.set("cpu_busy", (100 - idle_val).round(1))
                    end
                end

                event.set("cpu_core_count", individual_count) if individual_count > 0
            '
        }
    }
}

# Step 2: Build Splunk HEC payload for gw_sys_stats events
# Uses format => "message" pattern (same as suricata) so cpu_cores_parsed
# serializes as a proper JSON array instead of a double-escaped string
filter {
    if "gw_sys_stats" in [tags] {
        ruby {
            id => "sys-stats-hec-payload"
            code => '
                require "json"

                event_data = {
                    "gateway" => event.get("gateway"),
                    "alias" => event.get("alias"),
                    "cpu_idle" => event.get("cpu_idle"),
                    "memory_free" => event.get("memory_free"),
                    "memory_available" => event.get("memory_available"),
                    "memory_total" => event.get("memory_total"),
                    "disk_total" => event.get("disk_total"),
                    "disk_free" => event.get("disk_free"),
                    "syslog" => event.get("message")
                }

                # Add cpu_cores fields only if they exist (gateway events, not controller)
                cpu_busy = event.get("cpu_busy")
                event_data["cpu_busy"] = cpu_busy if cpu_busy

                cpu_core_count = event.get("cpu_core_count")
                event_data["cpu_core_count"] = cpu_core_count if cpu_core_count

                cpu_cores_parsed = event.get("cpu_cores_parsed")
                event_data["cpu_cores_parsed"] = cpu_cores_parsed if cpu_cores_parsed

                cpu_cores_raw = event.get("cpu_cores")
                event_data["cpu_cores_raw"] = cpu_cores_raw if cpu_cores_raw

                agg_min = event.get("cpu_aggregate_busy_min")
                event_data["cpu_aggregate_busy_min"] = agg_min if agg_min

                agg_max = event.get("cpu_aggregate_busy_max")
                event_data["cpu_aggregate_busy_max"] = agg_max if agg_max

                agg_avg = event.get("cpu_aggregate_busy_avg")
                event_data["cpu_aggregate_busy_avg"] = agg_avg if agg_avg

                # Remove nil values
                event_data.delete_if { |_k, v| v.nil? }

                payload = {
                    "sourcetype" => "aviatrix:gateway:system",
                    "source" => "avx-gw-sys-stats",
                    "host" => event.get("gateway"),
                    "time" => event.get("unix_time"),
                    "event" => event_data
                }

                event.set("[@metadata][sys_stats_hec_payload]", payload.to_json)
            '
        }
    }
}

# Microseg Throttling Filter
# Limits log volume for L4 microseg events to max 2 logs/minute per connection
# This reduces storage costs while maintaining visibility

filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Throttle key includes policy UUID, IPs, ports, and protocol
        # This ensures unique connections are tracked independently
        # Note: Uses dst_ip (correct field name from grok pattern)
        throttle {
            id => "microseg-throttle"
            key => "%{uuid}%{src_ip}%{dst_ip}%{src_port}%{dst_port}%{proto}"
            max_age => 120
            period => "60"
            after_count => 1
            add_tag => "throttled"
        }
    }
}

# Drop throttled events
filter {
    if "throttled" in [tags] {
        drop {
            id => "microseg-throttled"
        }
    }
}

# Timestamp Normalization Filter
# Converts the parsed date field to @timestamp and adds unix_time

filter {
    date {
        id => "date-to-timestamp"
        # Supports multiple timestamp formats:
        # - ISO8601: 2022-05-14T03:46:10.257442+00:00
        # - Long format: 2022-05-14 03:46:10.257442
        # - Short format with double space: Dec  9 03:46:16
        # - Short format with single space: Dec 14 03:46:16
        match => [ "date", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSSSSS", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        target => "@timestamp"
        remove_field => [ "date" ]
    }

    # Add unix timestamp for outputs that require epoch time
    ruby {
        id => "add-unix-time"
        code => "event.set('unix_time', event.get('@timestamp').to_i)"
    }
}

# Field Type Conversion Filter
# Converts string fields to their proper types for downstream processing

# Microseg field conversions
filter {
    if "microseg" in [tags] {
        mutate {
            id => "microseg-field-conversion"
            convert => {
                "src_port" => "integer"
                "dst_port" => "integer"
                "enforced" => "boolean"
            }
        }
    }
}

# Gateway network stats field conversions
filter {
    if "gw_net_stats" in [tags] {
        mutate {
            id => "gw_net_stats-rate-conversion"
            convert => {
                "total_rx_rate" => "integer"
                "total_tx_rate" => "integer"
                "total_rx_tx_rate" => "integer"
            }
            gsub => [
                "total_rx_rate", "Kb", "000",
                "total_tx_rate", "Kb", "000",
                "total_rx_tx_rate", "Kb", "000"
            ]
        }
    }
}

# Gateway system stats field conversions
filter {
    if "gw_sys_stats" in [tags] {
        mutate {
            id => "gw_sys_stats-field-conversion"
            convert => {
                "cpu_idle" => "float"
                "memory_free" => "integer"
                "memory_available" => "integer"
                "memory_total" => "integer"
                "disk_total" => "integer"
                "disk_free" => "integer"
            }
        }
    }
}

# ============================================================================
# OUTPUT CONFIGURATION (azure-log-ingestion)
# ============================================================================

# Azure Log Analytics / Sentinel Output (via Data Collection Rules)
# Uses the Microsoft Sentinel Log Analytics Logstash output plugin
#
# Supported Log Types:
#   - Suricata IDS alerts → Custom-AviatrixSuricata_CL
#   - L7 MITM/DCF (TLS inspection) → Custom-AviatrixMicroseg_CL
#   - L4 Microsegmentation → Custom-AviatrixMicroseg_CL
#
# Environment Variables:
#   client_app_id           - Azure AD application (service principal) ID
#   client_app_secret       - Azure AD application secret
#   tenant_id               - Azure AD tenant ID
#   data_collection_endpoint - Data Collection Endpoint URL
#   azure_dcr_suricata_id   - DCR immutable ID for Suricata logs
#   azure_stream_suricata   - Stream name for Suricata logs
#   azure_dcr_microseg_id   - DCR immutable ID for Microseg/MITM logs
#   azure_stream_microseg   - Stream name for Microseg/MITM logs
#   azure_cloud             - "AzureCloud", "AzureChinaCloud", or "AzureUSGovernment"

# Suricata-specific pre-processing for Azure
filter {
    if "suricata" in [tags] {
        mutate {
            id => "suricata-azure-cleanup"
            remove_field => ["message", "suricataData", "gw_hostname", "host", "port", "type", "event"]
        }

        # Add TimeGenerated field and flatten suricataDataJson for Azure
        ruby {
            id => "suricata-azure-flatten"
            code => "
                if event.get('suricataDataJson')
                    # Add TimeGenerated field required by Azure
                    event.set('TimeGenerated', event.get('@timestamp'))

                    # Flatten suricataDataJson into the event root
                    event.get('suricataDataJson').each { |k, v| event.set(k, v) }
                    event.remove('suricataDataJson')
                end
            "
        }
    }
}

# Microseg pre-processing for Azure
filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Add TimeGenerated field for Azure
        ruby {
            id => "microseg-azure-timegen"
            code => "event.set('TimeGenerated', event.get('@timestamp'))"
        }
    }
}

# MITM/L7 DCF pre-processing for Azure
filter {
    if "mitm" in [tags] {
        # Add TimeGenerated field for Azure
        ruby {
            id => "mitm-azure-timegen"
            code => "event.set('TimeGenerated', event.get('@timestamp'))"
        }
    }
}

output {
    # DEBUG: Output all events to stdout to verify processing
    stdout {
        id => "debug-stdout"
        codec => rubydebug {
            metadata => true
        }
    }

    # Suricata IDS events to Azure
    if "suricata" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-suricata"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_suricata_id}"
            dcr_stream_name => "${azure_stream_suricata}"
            azure_cloud => "${azure_cloud}"
        }
    }

    # L7 DCF / MITM events to Azure (TLS inspection)
    # MITM events are also tagged with "microseg" so they'll go to both outputs
    else if "mitm" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-mitm"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_microseg_id}"
            dcr_stream_name => "${azure_stream_microseg}"
            azure_cloud => "${azure_cloud}"
        }
    }

    # L4 Microseg events to Azure
    else if "microseg" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-microseg"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_microseg_id}"
            dcr_stream_name => "${azure_stream_microseg}"
            azure_cloud => "${azure_cloud}"
        }
    }
}
