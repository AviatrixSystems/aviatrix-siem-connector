# Aviatrix Log Integration Engine - Assembled Configuration
# Output Type: azure-log-ingestion
# Generated: 2025-12-08 18:33:15 UTC
#
# This file was automatically generated by assemble-config.sh
# Do not edit directly - modify the source modules instead:
#   - logstash-configs/inputs/
#   - logstash-configs/filters/
#   - logstash-configs/outputs/azure-log-ingestion/
#
# To regenerate: ./scripts/assemble-config.sh azure-log-ingestion

# ============================================================================
# INPUT CONFIGURATION
# ============================================================================

# Syslog Input Configuration
# Receives Aviatrix syslog messages on UDP/TCP port 5000

input {
    udp {
        port => 5000
        type => syslog
    }
    tcp {
        port => 5000
        type => syslog
    }
}

# ============================================================================
# FILTER CONFIGURATION
# ============================================================================

# FQDN Firewall Rule Filter
# Parses AviatrixFQDNRule syslog messages

filter {
    if [type] == "syslog" {
        grok {
            id => "fqdn"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["fqdn"]
            break_on_match => true
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*"
                ]
            }
        }
    }
}

# Controller CMD/API Filter
# Parses AviatrixCMD (V1) and AviatrixAPI (V2.5) syslog messages

# V1 API format (AviatrixCMD)
filter {
    if [type] == "syslog" and !("fqdn" in [tags]) {
        grok {
            id => "cmd-v1"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V1Api"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixCMD.*action=%{WORD:action}, argv=%{GREEDYDATA:args}, result=%{GREEDYDATA:result}, reason=%{GREEDYDATA:reason}, username=%{GREEDYDATA:username}",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixCMD.*action=%{WORD:action}, argv=%{GREEDYDATA:args}, result=%{GREEDYDATA:result}, reason=%{GREEDYDATA:reason}"
                ]
            }
        }
    }
}

# V2.5 API format (AviatrixAPI)
filter {
    if [type] == "syslog" and !("fqdn" in [tags]) {
        grok {
            id => "cmd-v2"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V2.5API"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixAPI.*url=%{GREEDYDATA:action} user=%{GREEDYDATA:username}? req_data=%{GREEDYDATA:args} resp_status=%{GREEDYDATA:result} resp_data=%{GREEDYDATA:reason}"
                ]
            }
        }
    }
}

# L4 Microsegmentation (eBPF) Filter
# Parses AviatrixGwMicrosegPacket syslog messages
# Supports both legacy 7.x and 8.2+ formats with session fields

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags]) {
        grok {
            id => "microseg"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["microseg", "ebpf"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    # 8.2+ format with session fields (end-of-session logs)
                    "^<%{NUMBER:syslog_pri}>%{SPACE}(%{MONTH} +%{MONTHDAY} +%{TIME} +%{HOSTNAME}-%{IP} syslog )?%{SYSLOG_TIMESTAMP:date} +GW-%{HOSTNAME:gw_hostname}-%{IP} +%{PATH}(\[%{NUMBER}\]:)? +%{YEAR}\/%{SPACE}%{MONTHNUM}\/%{SPACE}%{MONTHDAY} +%{TIME} +AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE} ACT=%{WORD:action} ENFORCED=%{WORD:enforced} SESSION_EVENT=%{NUMBER:session_event} SESSION_END_REASON=%{NUMBER:session_end_reason} SESSION_PACKET_COUNT=%{NUMBER:session_packet_count} SESSION_BYTE_COUNT=%{NUMBER:session_byte_count} SESSION_DURATION=%{NUMBER:session_duration_ns}",

                    # Legacy format with ACT and ENFORCED fields
                    "^<%{NUMBER}>%{SPACE}(%{MONTH} +%{MONTHDAY} +%{TIME} +%{HOSTNAME}-%{IP} syslog )?%{SYSLOG_TIMESTAMP:date} +GW-%{HOSTNAME:gw_hostname}-%{IP} +%{PATH}(\[%{NUMBER}\]:)? +%{YEAR}\/%{SPACE}%{MONTHNUM}\/%{SPACE}%{MONTHDAY} +%{TIME} +AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{GREEDYDATA} ACT=%{WORD:action} ENFORCED=%{WORD:enforced}",

                    # Older format without ACT/ENFORCED fields
                    "^<%{NUMBER}>%{SPACE}(%{MONTH} +%{MONTHDAY} +%{TIME} +%{HOSTNAME}-%{IP} syslog )?%{SYSLOG_TIMESTAMP:date} +GW-%{HOSTNAME:gw_hostname}-%{IP} +%{PATH}(\[%{NUMBER}\]:)? +%{YEAR}\/%{SPACE}%{MONTHNUM}\/%{SPACE}%{MONTHDAY} +%{TIME} +AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{GREEDYDATA}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }
    }
}

# L7 DCF / MITM (TLS Inspection) Filter
# Parses traffic_server JSON syslog messages and converts to microseg format
# Also creates cloned FQDN events for URL/hostname tracking

filter {
    if [type] == "syslog" and !("fqdn" in [tags] or "cmd" in [tags] or "microseg" in [tags]) {
        grok {
            id => "mitm"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["mitm"]
            remove_tag => ["_grokparsefailure"]
            match => {
                "message" => [
                    "^<%{NUMBER}>%{SPACE}(%{MONTH} +%{MONTHDAY} +%{TIME} +%{HOSTNAME}-%{IP} syslog )?%{MONTH} +%{MONTHDAY} +%{TIME} +GW-%{HOSTNAME:gw_hostname}-%{IP} traffic_server(\[%{NUMBER}\]:)? %{GREEDYDATA:[@metadata][json_payload]}$"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }

        # Parse JSON payload
        if "mitm" in [tags] {
            json {
                id => "mitm-json"
                skip_on_invalid_json => true
                source => "[@metadata][json_payload]"
                target => "[@metadata][payload]"
            }
        }

        # Convert MITM to microseg format
        if "mitm" in [tags] and [@metadata][payload] and "_jsonparsefailure" not in [tags] {
            # Use timestamp provided by MITM instead of syslog timestamp
            date {
                id => "mitm-timestamp"
                match => [ "[@metadata][payload][timestamp]", "UNIX" ]
                target => "@timestamp"
                remove_field => "date"
            }

            # Map MITM fields to microseg fields
            mutate {
                id => "mitm-map-to-microseg"
                add_field => {
                    "proto" => "TCP"
                    "action" => "%{[@metadata][payload][action]}"
                    "src_ip" => "%{[@metadata][payload][src]}"
                    "src_port" => "%{[@metadata][payload][src_port]}"
                    "dst_ip" => "%{[@metadata][payload][dest]}"
                    "dst_port" => "%{[@metadata][payload][dest_port]}"
                    "enforced" => "%{[@metadata][payload][enforced]}"
                    "uuid" => "%{[@metadata][payload][decided_by]}"
                    "mitm_sni_hostname" => "%{[@metadata][payload][sni_hostname]}"
                }
            }

            # Normalize DROP action to DENY
            if [@metadata][payload][action] == "DROP" {
                mutate {
                    id => "mitm-map-drop-to-deny"
                    replace => {
                        "[@metadata][payload][action]" => "DENY"
                    }
                }
            }

            # Add URL if present
            if [@metadata][payload][url] {
                mutate {
                    id => "mitm-url-parts"
                    add_field => {
                        "mitm_url_parts" => "%{[@metadata][payload][url]}"
                    }
                }
            }

            # Add decrypted_by if present
            if [@metadata][payload][decrypted_by] {
                mutate {
                    id => "mitm-decrypted-by"
                    add_field => {
                        "mitm_decrypted_by" => "%{[@metadata][payload][decrypted_by]}"
                    }
                }
            }

            # Add the microseg tag for output routing
            mutate {
                id => "mitm-add-microseg-tag"
                add_tag => ["microseg"]
            }

            # Clone event for FQDN tracking
            clone {
                clones => ["fqdn"]
                add_tag => ["fqdn"]
            }
        }
    }
}

# Transform cloned MITM events into FQDN format
filter {
    if "fqdn" in [tags] and "mitm" in [tags] and "microseg" in [tags] {
        if [@metadata][payload][url] {
            mutate {
                id => "fqdn-mitm-add-url"
                add_field => {
                    "url" => "%{[@metadata][payload][url]}"
                }
            }
        }

        # Map MITM microseg fields to FQDN fields
        mutate {
            id => "fqdn-mitm-map-to-fqdn"
            add_field => {
                "sip" => "%{[@metadata][payload][src]}"
                "dip" => "%{[@metadata][payload][dest]}"
                "gateway" => "%{[gw_hostname]}"
                "state" => "MATCHED"
                "hostname" => "%{[@metadata][payload][sni_hostname]}"
                "rule" => "%{[@metadata][payload][sni_hostname]};%{[@metadata][payload][dest_port]}"
            }
            remove_tag => ["microseg", "mitm"]
            remove_field => ["mitm_url_parts", "mitm_decrypted_by", "mitm_sni_hostname", "src_ip", "src_port", "dst_ip", "dst_port", "enforced", "gw_hostname"]
        }
    }
}

# Suricata IDS/IPS Filter
# Parses Suricata JSON syslog messages from Aviatrix gateways

filter {
    if [type] == "syslog" {
        grok {
            id => "suricata"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["suricata"]
            match => {
                "message" => [
                    "^<%{NUMBER}>%{SPACE}(%{MONTH} +%{MONTHDAY} +%{TIME} +%{HOSTNAME}-%{IP} syslog )?%{SYSLOG_TIMESTAMP:date} +%{HOSTNAME:gw_hostname}-%{IP} suricata(\[%{NUMBER}\]:)? %{GREEDYDATA:suricataData}"
                ]
            }
        }

        # Parse JSON data if it looks like valid JSON
        if "suricata" in [tags] and [suricataData] =~ "\A\{.+\}\z" {
            json {
                id => "suricata-data"
                skip_on_invalid_json => true
                source => "suricataData"
                target => "suricataDataJson"
            }
        }

        # Drop events that failed JSON parsing
        if "_jsonparsefailure" in [tags] {
            drop {
                id => "suricata-json-failure-drop"
            }
        }

        # Drop Suricata stats events (too verbose, not security relevant)
        if [suricataDataJson][event_type] == "stats" {
            drop {
                id => "suricata-stats-drop"
            }
        }
    }
}

# Gateway Performance Statistics Filter
# Parses AviatrixGwNetStats and AviatrixGwSysStats syslog messages

# Network statistics (interface throughput)
filter {
    if [type] == "syslog" {
        grok {
            id => "gw_net_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_net_stats"]
            break_on_match => true
            match => {
                "message" => [
                    "^%{SYSLOG_TIMESTAMP:date}.*AviatrixGwNetStats.*name=%{HOSTNAME:gateway}.*public_ip=%{IP:public_ip}.*private_ip=%{IP:private_ip}.*interface=%{WORD:interface}.*total_rx_rate=%{NOTSPACE:total_rx_rate}.*total_tx_rate=%{NOTSPACE:total_tx_rate}.*total_rx_tx_rate=%{NOTSPACE:total_rx_tx_rate}.*total_rx_cum=%{NOTSPACE:total_rx_cum}.*total_tx_cum=%{NOTSPACE:total_tx_cum}.*total_rx_tx_cum=%{NOTSPACE:total_rx_tx_cum}.*"
                ]
            }
        }
    }
}

# System statistics (CPU, memory, disk)
filter {
    if [type] == "syslog" {
        grok {
            id => "gw_sys_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_sys_stats"]
            break_on_match => true
            match => {
                "message" => [
                    "^%{SYSLOG_TIMESTAMP:date}.*AviatrixGwSysStats.*name=%{HOSTNAME:gateway}.*cpu_idle=%{NUMBER:cpu_idle}.*memory_free=%{NUMBER:memory_free}.*memory_available=%{NUMBER:memory_available}.*memory_total=%{NUMBER:memory_total}.*disk_total=%{NUMBER:disk_total}.*disk_free=%{NUMBER:disk_free}.*"
                ]
            }
        }
    }
}

# Tunnel Status Change Filter
# Parses AviatrixTunnelStatusChange syslog messages

filter {
    if [type] == "syslog" {
        grok {
            id => "tunnel_status"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["tunnel_status"]
            break_on_match => true
            match => {
                "message" => [
                    "^%{SYSLOG_TIMESTAMP:date}.*AviatrixTunnelStatusChange.*src_gw=%{TUNNEL_GW:src_gw}.*dst_gw=%{TUNNEL_GW:dst_gw}.*old_state=%{WORD:old_state}.*new_state=%{WORD:new_state}.*"
                ]
            }
        }
    }
}

# Microseg Throttling Filter
# Limits log volume for L4 microseg events to max 2 logs/minute per connection
# This reduces storage costs while maintaining visibility

filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Throttle key includes policy UUID, IPs, ports, and protocol
        # This ensures unique connections are tracked independently
        # Note: Uses dst_ip (correct field name from grok pattern)
        throttle {
            id => "microseg-throttle"
            key => "%{uuid}%{src_ip}%{dst_ip}%{src_port}%{dst_port}%{proto}"
            max_age => 120
            period => "60"
            after_count => 1
            add_tag => "throttled"
        }
    }
}

# Drop throttled events
filter {
    if "throttled" in [tags] {
        drop {
            id => "microseg-throttled"
        }
    }
}

# Timestamp Normalization Filter
# Converts the parsed date field to @timestamp and adds unix_time

filter {
    date {
        id => "date-to-timestamp"
        # Supports multiple timestamp formats:
        # - ISO8601: 2022-05-14T03:46:10.257442+00:00
        # - Long format: 2022-05-14 03:46:10.257442
        # - Short format: May 14 03:46:16
        match => [ "date", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSSSSS", "MMM dd HH:mm:ss" ]
        target => "@timestamp"
        remove_field => [ "date" ]
    }

    # Add unix timestamp for outputs that require epoch time
    ruby {
        id => "add-unix-time"
        code => "event.set('unix_time', event.get('@timestamp').to_i)"
    }
}

# Field Type Conversion Filter
# Converts string fields to their proper types for downstream processing

# Microseg field conversions
filter {
    if "microseg" in [tags] {
        mutate {
            id => "microseg-field-conversion"
            convert => {
                "src_port" => "integer"
                "dst_port" => "integer"
                "enforced" => "boolean"
            }
        }
    }
}

# Gateway network stats field conversions
filter {
    if "gw_net_stats" in [tags] {
        mutate {
            id => "gw_net_stats-rate-conversion"
            convert => {
                "total_rx_rate" => "integer"
                "total_tx_rate" => "integer"
                "total_rx_tx_rate" => "integer"
            }
            gsub => [
                "total_rx_rate", "Kb", "000",
                "total_tx_rate", "Kb", "000",
                "total_rx_tx_rate", "Kb", "000"
            ]
        }
    }
}

# Gateway system stats field conversions
filter {
    if "gw_sys_stats" in [tags] {
        mutate {
            id => "gw_sys_stats-field-conversion"
            convert => {
                "cpu_idle" => "float"
                "memory_free" => "integer"
                "memory_available" => "integer"
                "memory_total" => "integer"
                "disk_total" => "integer"
                "disk_free" => "integer"
            }
        }
    }
}

# ============================================================================
# OUTPUT CONFIGURATION (azure-log-ingestion)
# ============================================================================

# Azure Log Analytics / Sentinel Output (via Data Collection Rules)
# Uses the Microsoft Sentinel Log Analytics Logstash output plugin
#
# Environment Variables:
#   client_app_id           - Azure AD application (service principal) ID
#   client_app_secret       - Azure AD application secret
#   tenant_id               - Azure AD tenant ID
#   data_collection_endpoint - Data Collection Endpoint URL
#   azure_dcr_suricata_id   - DCR immutable ID for Suricata logs
#   azure_stream_suricata   - Stream name for Suricata logs
#   azure_dcr_microseg_id   - DCR immutable ID for Microseg logs
#   azure_stream_microseg   - Stream name for Microseg logs
#   azure_cloud             - "public" or "china"

# Suricata-specific pre-processing for Azure
filter {
    if "suricata" in [tags] {
        mutate {
            id => "suricata-azure-cleanup"
            remove_field => ["message", "suricataData", "gw_hostname", "host", "port", "type", "event"]
        }

        # Add TimeGenerated field and flatten suricataDataJson for Azure
        ruby {
            id => "suricata-azure-flatten"
            code => "
                if event.get('suricataDataJson')
                    # Add TimeGenerated field required by Azure
                    event.set('TimeGenerated', event.get('@timestamp'))

                    # Flatten suricataDataJson into the event root
                    event.get('suricataDataJson').each { |k, v| event.set(k, v) }
                    event.remove('suricataDataJson')
                end
            "
        }
    }
}

# Microseg pre-processing for Azure
filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Add TimeGenerated field for Azure
        ruby {
            id => "microseg-azure-timegen"
            code => "event.set('TimeGenerated', event.get('@timestamp'))"
        }
    }
}

output {
    # Suricata IDS events to Azure
    if "suricata" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-suricata"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_suricata_id}"
            dcr_stream_name => "${azure_stream_suricata}"
            azure_cloud => "${azure_cloud}"
        }
    }

    # L4 Microseg events to Azure
    else if "microseg" in [tags] {
        microsoft-sentinel-log-analytics-logstash-output-plugin {
            id => "azure-microseg"
            client_app_Id => "${client_app_id}"
            client_app_secret => "${client_app_secret}"
            tenant_id => "${tenant_id}"
            data_collection_endpoint => "${data_collection_endpoint}"
            dcr_immutable_id => "${azure_dcr_microseg_id}"
            dcr_stream_name => "${azure_stream_microseg}"
            azure_cloud => "${azure_cloud}"
        }
    }
}
