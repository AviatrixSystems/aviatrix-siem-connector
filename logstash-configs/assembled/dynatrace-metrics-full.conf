# Aviatrix Log Integration Engine - Assembled Configuration
# Output Type: dynatrace-metrics
# Generated: 2026-02-15 19:50:26 UTC
#
# This file was automatically generated by assemble-config.sh
# Do not edit directly - modify the source modules instead:
#   - logstash-configs/inputs/
#   - logstash-configs/filters/
#   - logstash-configs/outputs/dynatrace-metrics/
#
# To regenerate: ./scripts/assemble-config.sh dynatrace-metrics

# ============================================================================
# INPUT CONFIGURATION
# ============================================================================

# Syslog Input Configuration
# Receives Aviatrix syslog messages on UDP/TCP port 5000

input {
    udp {
        port => 5000
        type => syslog
    }
    tcp {
        port => 5000
        type => syslog
    }
}

# ============================================================================
# FILTER CONFIGURATION
# ============================================================================

# L4 Microsegmentation (eBPF) Filter
# Parses AviatrixGwMicrosegPacket syslog messages
# Supports both legacy 7.x and 8.2+ formats with session fields

filter {
    if [type] == "syslog" and "AviatrixGwMicrosegPacket" in [message] {
        grok {
            id => "microseg"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["microseg", "ebpf"]
            tag_on_failure => []
            match => {
                "message" => [
                    # 8.2+ format: GW-<name>-<ip>-sink with /usr/local/bin/avx-gw-state-sync process and session fields
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP:gw_ip}%{DATA}AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE:data_hex} ACT=%{WORD:action} ENFORCED=%{WORD:enforced} SESSION_ID=%{NUMBER:session_id} SESSION_EVENT=%{NUMBER:session_event} SESSION_END_REASON=%{NUMBER:session_end_reason} SESSION_PKT_CNT=%{NUMBER:session_pkt_cnt} SESSION_BYTE_CNT=%{NUMBER:session_byte_cnt} SESSION_DUR=%{NUMBER:session_dur}",

                    # 8.2+ format without session fields (first packet of flow)
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP:gw_ip}%{DATA}AviatrixGwMicrosegPacket: POLICY=%{UUID:uuid} SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} IP_SZ=%{NUMBER:ip_size} SRC_IP=%{IP:src_ip} DST_IP=%{IP:dst_ip} PROTO=%{WORD:proto} SRC_PORT=%{NUMBER:src_port} DST_PORT=%{NUMBER:dst_port} DATA=%{NOTSPACE:data_hex} ACT=%{WORD:action} ENFORCED=%{WORD:enforced}",

                    # Legacy format (SPT/DPT/ACTION)
                    "^<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +%{HOSTNAME:gw_hostname} +AviatrixGwMicrosegPacket: SRC_MAC=%{MAC:src_mac} DST_MAC=%{MAC:dst_mac} PROTO=%{WORD:proto} SPT=%{NUMBER:src_port} DPT=%{NUMBER:dst_port} ACTION=%{WORD:action}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }
    }
}

# Set default values for fields missing in legacy microseg format
filter {
    if "microseg" in [tags] and ![src_ip] {
        mutate {
            id => "microseg-legacy-defaults"
            add_field => {
                "src_ip" => "unknown"
                "dst_ip" => "unknown"
                "uuid" => "legacy-format"
                "enforced" => "unknown"
            }
        }
    }
}

# L7 DCF / MITM (TLS Inspection) Filter
# Parses traffic_server JSON syslog messages

filter {
    if [type] == "syslog" and "traffic_server[" in [message] {
        grok {
            id => "mitm"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["mitm"]
            tag_on_failure => []
            match => {
                "message" => [
                    # traffic_server JSON format with syslog priority
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP}%{DATA}traffic_server\[%{NUMBER}\]: %{GREEDYDATA:[@metadata][json_payload]}"
                ]
            }
            remove_field => ["event", "@version", "type", "host"]
        }

        # Parse JSON payload
        if "mitm" in [tags] {
            json {
                id => "mitm-json"
                skip_on_invalid_json => true
                source => "[@metadata][json_payload]"
                target => "[@metadata][payload]"
            }
        }

        # Convert MITM to microseg format
        if "mitm" in [tags] and [@metadata][payload] and "_jsonparsefailure" not in [tags] {
            # Use timestamp provided by MITM instead of syslog timestamp
            date {
                id => "mitm-timestamp"
                match => [ "[@metadata][payload][timestamp]", "UNIX" ]
                target => "@timestamp"
                remove_field => "date"
            }

            # Map MITM fields to microseg fields
            mutate {
                id => "mitm-map-to-microseg"
                add_field => {
                    "proto" => "TCP"
                    "action" => "%{[@metadata][payload][action]}"
                    "src_ip" => "%{[@metadata][payload][src]}"
                    "src_port" => "%{[@metadata][payload][src_port]}"
                    "dst_ip" => "%{[@metadata][payload][dest]}"
                    "dst_port" => "%{[@metadata][payload][dest_port]}"
                    "enforced" => "%{[@metadata][payload][enforced]}"
                    "uuid" => "%{[@metadata][payload][decided_by]}"
                    "mitm_sni_hostname" => "%{[@metadata][payload][sni_hostname]}"
                }
            }

            # Normalize DROP action to DENY
            if [@metadata][payload][action] == "DROP" {
                mutate {
                    id => "mitm-map-drop-to-deny"
                    replace => {
                        "[@metadata][payload][action]" => "DENY"
                    }
                }
            }

            # Add URL if present
            if [@metadata][payload][url] {
                mutate {
                    id => "mitm-url-parts"
                    add_field => {
                        "mitm_url_parts" => "%{[@metadata][payload][url]}"
                    }
                }
            }

            # Add decrypted_by if present
            if [@metadata][payload][decrypted_by] {
                mutate {
                    id => "mitm-decrypted-by"
                    add_field => {
                        "mitm_decrypted_by" => "%{[@metadata][payload][decrypted_by]}"
                    }
                }
            }

        }
    }
}

# Suricata IDS/IPS Filter
# Parses Suricata JSON syslog messages from Aviatrix gateways
#
# Behavior controlled by FLATTEN_SURICATA environment variable:
#   FLATTEN_SURICATA=true  → Flattens nested objects for Splunk (recommended)
#   FLATTEN_SURICATA unset → Keeps nested JSON structure for Azure/other outputs
#
# When flattening is enabled:
#   alert.*    → top level (signature, severity, category, etc.)
#   flow.*     → flow_* (pkts_toserver, bytes_toclient, etc.)
#   http.*     → http_* (hostname, url, method, status, user_agent)
#   tls.*      → tls_* (sni, subject, issuer, ja3, fingerprint)
#   dns.*      → dns_* (query, rrname, rrtype, rdata, ttl)
#   smtp.*     → smtp_* (mail_from, rcpt_to, subject)
#   ssh.*      → ssh_client_*, ssh_server_* (software_version, hassh)
#   fileinfo.* → file_* (filename, size, md5, sha256)
#   tcp.*      → tcp_* (state, tcp_flags)
#   metadata.* → meta_* (confidence, signature_severity - arrays take first value)
#
# Always drops high-volume/low-value fields: files, payload, payload_printable, packet

filter {
    if [type] == "syslog" and "suricata[" in [message] {
        grok {
            id => "suricata"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["suricata"]
            tag_on_failure => []
            match => {
                "message" => [
                    # Suricata JSON alert format with syslog priority
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date} +GW-%{DATA:gw_hostname}-%{IP}%{DATA}suricata\[%{NUMBER}\]: %{GREEDYDATA:suricataData}"
                ]
            }
        }

        # Parse JSON data if it looks like valid JSON (starts with { and contains "event_type")
        if "suricata" in [tags] and [suricataData] =~ /^\{.*"event_type"/ {
            json {
                id => "suricata-data"
                skip_on_invalid_json => true
                source => "suricataData"
                target => "suricataDataJson"
            }
        }

        # Drop non-JSON suricata logs (notices, startup messages)
        if "suricata" in [tags] and !([suricataData] =~ /^\{/) {
            drop {
                id => "suricata-non-json-drop"
            }
        }

        # Drop events that failed JSON parsing
        if "_jsonparsefailure" in [tags] {
            drop {
                id => "suricata-json-failure-drop"
            }
        }

        # Drop Suricata stats events (too verbose, not security relevant)
        if [suricataDataJson][event_type] == "stats" {
            drop {
                id => "suricata-stats-drop"
            }
        }

        # Process Suricata JSON data
        # When FLATTEN_SURICATA=true: Flattens all nested objects for Splunk
        # When FLATTEN_SURICATA!=true: Keeps nested structure for Azure/other outputs
        if "suricata" in [tags] and [suricataDataJson] {
            ruby {
                id => "suricata-process"
                init => '
                    require "json"
                    require "time"

                    def flatten_object(data, obj_name, prefix, skip_keys = [])
                        return {} unless data[obj_name].is_a?(Hash)
                        result = {}
                        data[obj_name].each do |k, v|
                            next if skip_keys.include?(k)
                            if v.is_a?(Hash)
                                v.each do |k2, v2|
                                    next if v2.is_a?(Hash) || v2.is_a?(Array)
                                    result["#{prefix}_#{k}_#{k2}"] = v2
                                end
                            elsif v.is_a?(Array)
                                result["#{prefix}_#{k}"] = v[0] if v[0] && !v[0].is_a?(Hash) && !v[0].is_a?(Array)
                            else
                                result["#{prefix}_#{k}"] = v
                            end
                        end
                        result
                    end
                '
                code => '
                    data = event.get("suricataDataJson")
                    next unless data.is_a?(Hash)

                    # Fields to drop (high-volume/low-value) - applies to all outputs
                    drop_fields = ["files", "payload", "payload_printable", "packet", "tx_guessed", "policy_id"]
                    tls_skip = ["certificate", "chain"]

                    # Calculate unix timestamp from Suricata timestamp
                    unix_time = nil
                    if data["timestamp"]
                        begin
                            unix_time = Time.parse(data["timestamp"]).to_i
                        rescue
                            unix_time = Time.now.to_i
                        end
                    else
                        unix_time = Time.now.to_i
                    end

                    # Check if flattening is enabled (for Splunk)
                    if ENV["FLATTEN_SURICATA"] == "true"
                        flattened = {}

                        # Nested objects to flatten with their prefixes
                        nested_objects = {
                            "flow" => "flow",
                            "http" => "http",
                            "tls" => "tls",
                            "dns" => "dns",
                            "smtp" => "smtp",
                            "ssh" => "ssh",
                            "fileinfo" => "file",
                            "tcp" => "tcp"
                        }

                        # Process each top-level field
                        data.each do |key, value|
                            next if drop_fields.include?(key)

                            if key == "alert" && value.is_a?(Hash)
                                # Flatten alert fields to top level (no prefix)
                                value.each do |alert_key, alert_value|
                                    next if alert_value.is_a?(Hash) || alert_value.is_a?(Array)
                                    flattened[alert_key] = alert_value
                                end
                            elsif key == "metadata" && value.is_a?(Hash)
                                # Handle metadata specially - arrays take first value, skip timestamps
                                value.each do |meta_key, meta_value|
                                    next if ["created_at", "updated_at"].include?(meta_key)
                                    if meta_value.is_a?(Array)
                                        flattened["meta_#{meta_key}"] = meta_value[0] if meta_value[0]
                                    elsif !meta_value.is_a?(Hash)
                                        flattened["meta_#{meta_key}"] = meta_value
                                    end
                                end
                            elsif nested_objects.key?(key)
                                # Flatten known nested objects with appropriate prefix
                                skip_keys = (key == "tls") ? tls_skip : []
                                flattened.merge!(flatten_object(data, key, nested_objects[key], skip_keys))
                            elsif !value.is_a?(Hash) && !value.is_a?(Array)
                                # Keep primitive top-level fields as-is
                                flattened[key] = value
                            end
                        end

                        # Fix http_ prefix duplication (http.http_method → http_method not http_http_method)
                        flattened.keys.select { |k| k.start_with?("http_http_") }.each do |k|
                            new_key = k.sub("http_http_", "http_")
                            flattened[new_key] = flattened.delete(k)
                        end

                        event_data = flattened
                    else
                        # Non-flattened mode: keep nested structure but remove unwanted fields
                        cleaned = data.dup
                        drop_fields.each { |f| cleaned.delete(f) }
                        if cleaned["tls"].is_a?(Hash)
                            tls_skip.each { |f| cleaned["tls"].delete(f) }
                        end
                        event_data = cleaned
                    end

                    # Build complete HEC payload with event as JSON object
                    payload = {
                        "sourcetype" => "aviatrix:ids",
                        "source" => "avx-ids",
                        "host" => event.get("gw_hostname"),
                        "time" => unix_time,
                        "event" => event_data
                    }
                    event.set("[@metadata][suricata_hec_payload]", payload.to_json)
                '
            }
        }
    }
}

# FQDN Firewall Rule Filter
# Parses AviatrixFQDNRule syslog messages

filter {
    if [type] == "syslog" and "AviatrixFQDNRule" in [message] {
        grok {
            id => "fqdn"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["fqdn"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*Rule=%{RULE:rule}.*",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*drop_reason=%{WORD:drop}",
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixFQDNRule.*Gateway=%{HOSTNAME:gateway}.*S_IP=%{IP:sip}.*D_IP=%{IP:dip}.*hostname=%{HOSTNAME:hostname}.*state=%{WORD:state}.*"
                ]
            }
        }
    }
}

# Controller CMD/API Filter
# Parses AviatrixCMD (V1) and AviatrixAPI (V2.5) syslog messages

# V1 API format (AviatrixCMD)
filter {
    if [type] == "syslog" and "AviatrixCMD" in [message] {
        grok {
            id => "cmd-v1"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V1Api"]
            tag_on_failure => []
            match => {
                "message" => [
                    # Format with username at end (reason may be empty)
                    "%{SYSLOG_TIMESTAMP:date}.*Controller-%{IP:controller_ip}.*AviatrixCMD: action=%{WORD:action}, argv=%{DATA:args}, result=%{WORD:result}, reason=%{DATA:reason}, username=%{NOTSPACE:username}",
                    # Format without username (reason may be empty)
                    "%{SYSLOG_TIMESTAMP:date}.*Controller-%{IP:controller_ip}.*AviatrixCMD: action=%{WORD:action}, argv=%{DATA:args}, result=%{WORD:result}, reason=%{DATA:reason}$"
                ]
            }
        }
    }
}

# Set gw_hostname for CMD logs (use controller_ip if available)
filter {
    if "cmd" in [tags] and [controller_ip] {
        mutate {
            id => "cmd-set-hostname"
            add_field => { "gw_hostname" => "Controller-%{controller_ip}" }
        }
    }
}

# Set default value for empty reason field in CMD logs
filter {
    if "cmd" in [tags] and (![reason] or [reason] == "") {
        mutate {
            id => "cmd-default-reason"
            replace => { "reason" => "" }
        }
    }
}

# V2.5 API format (AviatrixAPI)
filter {
    if [type] == "syslog" and "AviatrixAPI" in [message] and !("cmd" in [tags]) {
        grok {
            id => "cmd-v2"
            patterns_dir => ["/usr/share/logstash/patterns"]
            break_on_match => true
            add_tag => ["cmd", "V2.5API"]
            tag_on_failure => []
            match => {
                "message" => [
                    "%{SYSLOG_TIMESTAMP:date}.*AviatrixAPI.*url=%{GREEDYDATA:action} user=%{GREEDYDATA:username}? req_data=%{GREEDYDATA:args} resp_status=%{GREEDYDATA:result} resp_data=%{GREEDYDATA:reason}"
                ]
            }
        }
    }
}

# Gateway Performance Statistics Filter
# Parses AviatrixGwNetStats and AviatrixGwSysStats syslog messages

# Network statistics (interface throughput)
filter {
    if [type] == "syslog" and "AviatrixGwNetStats:" in [message] {
        grok {
            id => "gw_net_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_net_stats"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    # With syslog priority prefix and public_ip
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date}%{DATA}AviatrixGwNetStats:%{DATA}name=%{NOTSPACE:gateway}(?:%{DATA}alias=%{NOTSPACE:alias})?(?:%{DATA}public_ip=%{IP:public_ip})?%{DATA}private_ip=%{IP:private_ip}%{DATA}interface=%{NOTSPACE:interface}%{DATA}total_rx_rate=%{NOTSPACE:total_rx_rate}%{DATA}total_tx_rate=%{NOTSPACE:total_tx_rate}%{DATA}total_rx_tx_rate=%{NOTSPACE:total_rx_tx_rate}%{DATA}total_rx_cum=%{NOTSPACE:total_rx_cum}%{DATA}total_tx_cum=%{NOTSPACE:total_tx_cum}%{DATA}total_rx_tx_cum=%{NOTSPACE:total_rx_tx_cum}(?:%{DATA}conntrack_limit_exceeded=%{NUMBER:conntrack_limit_exceeded})?(?:%{DATA}bw_in_limit_exceeded=%{NUMBER:bw_in_limit_exceeded})?(?:%{DATA}bw_out_limit_exceeded=%{NUMBER:bw_out_limit_exceeded})?(?:%{DATA}pps_limit_exceeded=%{NUMBER:pps_limit_exceeded})?(?:%{DATA}linklocal_limit_exceeded=%{NUMBER:linklocal_limit_exceeded})?(?:%{DATA}conntrack_count=%{NUMBER:conntrack_count})?(?:%{DATA}conntrack_allowance_available=%{NUMBER:conntrack_allowance_available})?(?:%{DATA}conntrack_usage_rate=%{NUMBER:conntrack_usage_rate})?"
                ]
            }
        }
    }
}

# System statistics (CPU, memory, disk)
filter {
    if [type] == "syslog" and "AviatrixGwSysStats:" in [message] {
        grok {
            id => "gw_sys_stats"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["gw_sys_stats"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    "<%{NUMBER:syslog_pri}>%{SYSLOG_TIMESTAMP:date}%{DATA}AviatrixGwSysStats:%{DATA}name=%{NOTSPACE:gateway}(?:%{DATA}alias=%{NOTSPACE:alias})?%{DATA}cpu_idle=%{NUMBER:cpu_idle}%{DATA}memory_free=%{NUMBER:memory_free}%{DATA}memory_available=%{NUMBER:memory_available}%{DATA}memory_total=%{NUMBER:memory_total}%{DATA}disk_total=%{NUMBER:disk_total}%{DATA}disk_free=%{NUMBER:disk_free}(?:%{DATA}cpu_cores=%{GREEDYDATA:cpu_cores})?"
                ]
            }
        }
    }
}

# Tunnel Status Change Filter
# Parses AviatrixTunnelStatusChange syslog messages

filter {
    if [type] == "syslog" and "AviatrixTunnelStatusChange" in [message] {
        grok {
            id => "tunnel_status"
            patterns_dir => ["/usr/share/logstash/patterns"]
            add_tag => ["tunnel_status"]
            tag_on_failure => []
            break_on_match => true
            match => {
                "message" => [
                    "^%{SYSLOG_TIMESTAMP:date}.*AviatrixTunnelStatusChange.*src_gw=%{TUNNEL_GW:src_gw}.*dst_gw=%{TUNNEL_GW:dst_gw}.*old_state=%{WORD:old_state}.*new_state=%{WORD:new_state}.*"
                ]
            }
        }
    }
}

# CPU Cores Parser
# Parses protobuf-text cpu_cores field from AviatrixGwSysStats into structured JSON

# Step 1: Parse cpu_cores protobuf text into structured fields
filter {
    if "gw_sys_stats" in [tags] and [cpu_cores] and [cpu_cores] != "" {
        ruby {
            id => "cpu-cores-parse"
            code => '
                raw = event.get("cpu_cores")
                next unless raw.is_a?(String) && raw.length > 2

                # Strip outer brackets
                raw = raw.strip
                raw = raw[1..-2] if raw.start_with?("[") && raw.end_with?("]")

                # Tokenize into name:N and busy:{...} tokens with nested brace matching
                tokens = []
                i = 0
                while i < raw.length
                    # Skip whitespace
                    if raw[i] =~ /\s/
                        i += 1
                        next
                    end

                    # Match name:N token
                    if raw[i..] =~ /\Aname:(-?\d+)/
                        tokens << { type: :name, value: $1 }
                        i += $~[0].length
                        next
                    end

                    # Match busy:{...} token with nested brace matching
                    if raw[i..] =~ /\Abusy:\{/
                        depth = 0
                        start = i + 5  # skip "busy:"
                        j = start
                        while j < raw.length
                            if raw[j] == "{"
                                depth += 1
                            elsif raw[j] == "}"
                                depth -= 1
                                if depth == 0
                                    body = raw[start..j]
                                    tokens << { type: :busy, body: body }
                                    i = j + 1
                                    break
                                end
                            end
                            j += 1
                        end
                        # If we never closed braces, skip past what we matched
                        i = j + 1 if depth != 0
                        next
                    end

                    # Skip any unrecognized character
                    i += 1
                end

                next if tokens.empty?

                # Extract min/max/avg from a busy block body
                # Strips out start:{...} and end:{...} sub-blocks first
                extract_stats = lambda do |body|
                    # Remove nested start:{...} and end:{...} blocks
                    cleaned = body.gsub(/(?:start|end):\{[^}]*\}/, "")
                    stats = {}
                    cleaned.scan(/(min|max|avg):(\d+)/).each do |key, val|
                        stats[key] = val.to_i
                    end
                    stats
                end

                # Detect format: name-first vs busy-first
                name_first = tokens[0][:type] == :name

                # Pair tokens into core entries
                cores = []
                if name_first
                    pending_name = nil
                    tokens.each do |tok|
                        if tok[:type] == :name
                            pending_name = tok[:value]
                        elsif tok[:type] == :busy
                            stats = extract_stats.call(tok[:body])
                            entry = {}
                            entry["name"] = pending_name if pending_name
                            entry["busy_min"] = stats["min"] if stats["min"]
                            entry["busy_max"] = stats["max"] if stats["max"]
                            entry["busy_avg"] = stats["avg"] if stats["avg"]
                            cores << entry
                            pending_name = nil
                        end
                    end
                else
                    # busy-first: each busy optionally consumes the following name
                    idx = 0
                    while idx < tokens.length
                        tok = tokens[idx]
                        if tok[:type] == :busy
                            stats = extract_stats.call(tok[:body])
                            entry = {}
                            # Check if next token is a name
                            if idx + 1 < tokens.length && tokens[idx + 1][:type] == :name
                                entry["name"] = tokens[idx + 1][:value]
                                idx += 1
                            end
                            entry["busy_min"] = stats["min"] if stats["min"]
                            entry["busy_max"] = stats["max"] if stats["max"]
                            entry["busy_avg"] = stats["avg"] if stats["avg"]
                            cores << entry
                        elsif tok[:type] == :name
                            # Standalone name followed by busy
                            if idx + 1 < tokens.length && tokens[idx + 1][:type] == :busy
                                stats = extract_stats.call(tokens[idx + 1][:body])
                                entry = { "name" => tok[:value] }
                                entry["busy_min"] = stats["min"] if stats["min"]
                                entry["busy_max"] = stats["max"] if stats["max"]
                                entry["busy_avg"] = stats["avg"] if stats["avg"]
                                cores << entry
                                idx += 1
                            end
                        end
                        idx += 1
                    end
                end

                next if cores.empty?

                # Set parsed array
                event.set("cpu_cores_parsed", cores)

                # Extract aggregate (name=-1) and individual core stats
                aggregate = nil
                individual_count = 0
                cores.each do |c|
                    if c["name"] == "-1"
                        aggregate = c
                    elsif c["name"] && c["name"] != "-1"
                        individual_count += 1
                    end
                end

                if aggregate
                    event.set("cpu_aggregate_busy_min", aggregate["busy_min"]) if aggregate["busy_min"]
                    event.set("cpu_aggregate_busy_max", aggregate["busy_max"]) if aggregate["busy_max"]
                    event.set("cpu_aggregate_busy_avg", aggregate["busy_avg"]) if aggregate["busy_avg"]
                    # cpu_busy: prefer avg, fall back to max
                    if aggregate["busy_avg"]
                        event.set("cpu_busy", aggregate["busy_avg"])
                    elsif aggregate["busy_max"]
                        event.set("cpu_busy", aggregate["busy_max"])
                    end
                end

                # Fallback: compute cpu_busy from cpu_idle if no aggregate found
                if !aggregate
                    cpu_idle = event.get("cpu_idle")
                    if cpu_idle
                        idle_val = cpu_idle.is_a?(Numeric) ? cpu_idle : cpu_idle.to_f
                        event.set("cpu_busy", (100 - idle_val).round(1))
                    end
                end

                event.set("cpu_core_count", individual_count) if individual_count > 0
            '
        }
    }
}

# Microseg Throttling Filter
# Limits log volume for L4 microseg events to max 2 logs/minute per connection
# This reduces storage costs while maintaining visibility

filter {
    if "microseg" in [tags] and "mitm" not in [tags] {
        # Throttle key includes policy UUID, IPs, ports, and protocol
        # This ensures unique connections are tracked independently
        # Note: Uses dst_ip (correct field name from grok pattern)
        throttle {
            id => "microseg-throttle"
            key => "%{uuid}%{src_ip}%{dst_ip}%{src_port}%{dst_port}%{proto}"
            max_age => 120
            period => "60"
            after_count => 1
            add_tag => "throttled"
        }
    }
}

# Drop throttled events
filter {
    if "throttled" in [tags] {
        drop {
            id => "microseg-throttled"
        }
    }
}

# Timestamp Normalization Filter
# Converts the parsed date field to @timestamp and adds unix_time

filter {
    if [date] {
        date {
            id => "date-to-timestamp"
            # Supports multiple timestamp formats:
            # - ISO8601: 2022-05-14T03:46:10.257442+00:00
            # - Long format: 2022-05-14 03:46:10.257442
            # - Short format with double space: Dec  9 03:46:16
            # - Short format with single space: Dec 14 03:46:16
            match => [ "date", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSSSSS", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
            target => "@timestamp"
            remove_field => [ "date" ]
        }
    }

    # Add unix timestamp for outputs that require epoch time
    ruby {
        id => "add-unix-time"
        code => "event.set('unix_time', event.get('@timestamp').to_i)"
    }
}

# Save Raw Network Rate/Cumulative Strings (Dynatrace only)
# Preserves original string values (e.g. "54.07Kb", "2.49GB") in [@metadata]
# before filter 95 truncates them via integer conversion.
#
# The Dynatrace MINT builder needs these raw strings to compute accurate
# byte values (e.g. "54.07Kb" -> 55367.68 bytes/s).
#
# Only runs when DT_METRICS_URL is set (dynatrace-metrics output).

filter {
    if "gw_net_stats" in [tags] and "${DT_METRICS_URL:}" != "" {
        mutate {
            id => "save-raw-net-rates"
            copy => {
                "total_rx_rate" => "[@metadata][raw_total_rx_rate]"
                "total_tx_rate" => "[@metadata][raw_total_tx_rate]"
                "total_rx_tx_rate" => "[@metadata][raw_total_rx_tx_rate]"
                "total_rx_cum" => "[@metadata][raw_total_rx_cum]"
                "total_tx_cum" => "[@metadata][raw_total_tx_cum]"
                "total_rx_tx_cum" => "[@metadata][raw_total_rx_tx_cum]"
            }
        }
    }
}

# Field Type Conversion Filter
# Converts string fields to their proper types for downstream processing

# Microseg field conversions
filter {
    if "microseg" in [tags] {
        mutate {
            id => "microseg-field-conversion"
            convert => {
                "src_port" => "integer"
                "dst_port" => "integer"
                "enforced" => "boolean"
            }
        }
    }
}

# MITM field conversions (ports and enforced set as strings by mutate add_field)
filter {
    if "mitm" in [tags] {
        mutate {
            id => "mitm-field-conversion"
            convert => {
                "src_port" => "integer"
                "dst_port" => "integer"
                "enforced" => "boolean"
            }
        }
    }
}

# Gateway network stats field conversions
filter {
    if "gw_net_stats" in [tags] {
        mutate {
            id => "gw_net_stats-rate-conversion"
            convert => {
                "total_rx_rate" => "integer"
                "total_tx_rate" => "integer"
                "total_rx_tx_rate" => "integer"
            }
            gsub => [
                "total_rx_rate", "Kb", "000",
                "total_tx_rate", "Kb", "000",
                "total_rx_tx_rate", "Kb", "000"
            ]
        }
    }
}

# Gateway system stats field conversions
filter {
    if "gw_sys_stats" in [tags] {
        mutate {
            id => "gw_sys_stats-field-conversion"
            convert => {
                "cpu_idle" => "float"
                "memory_free" => "integer"
                "memory_available" => "integer"
                "memory_total" => "integer"
                "disk_total" => "integer"
                "disk_free" => "integer"
            }
        }
    }
}

# System Stats HEC Payload Builder
# Builds Splunk HEC payload for gw_sys_stats events (same pattern as suricata)
# Uses format => "message" so cpu_cores_parsed serializes as a proper JSON array
#
# Runs after 90-timestamp.conf (unix_time) and 95-field-conversion.conf (type coercions)

filter {
    if "gw_sys_stats" in [tags] {
        ruby {
            id => "sys-stats-hec-payload"
            code => '
                require "json"

                event_data = {
                    "gateway" => event.get("gateway"),
                    "alias" => event.get("alias"),
                    "cpu_idle" => event.get("cpu_idle"),
                    "memory_free" => event.get("memory_free"),
                    "memory_available" => event.get("memory_available"),
                    "memory_total" => event.get("memory_total"),
                    "disk_total" => event.get("disk_total"),
                    "disk_free" => event.get("disk_free"),
                    "syslog" => event.get("message")
                }

                # Add cpu_cores fields only if they exist (gateway events, not controller)
                cpu_busy = event.get("cpu_busy")
                event_data["cpu_busy"] = cpu_busy if cpu_busy

                cpu_core_count = event.get("cpu_core_count")
                event_data["cpu_core_count"] = cpu_core_count if cpu_core_count

                cpu_cores_parsed = event.get("cpu_cores_parsed")
                event_data["cpu_cores_parsed"] = cpu_cores_parsed if cpu_cores_parsed

                cpu_cores_raw = event.get("cpu_cores")
                event_data["cpu_cores_raw"] = cpu_cores_raw if cpu_cores_raw

                agg_min = event.get("cpu_aggregate_busy_min")
                event_data["cpu_aggregate_busy_min"] = agg_min if agg_min

                agg_max = event.get("cpu_aggregate_busy_max")
                event_data["cpu_aggregate_busy_max"] = agg_max if agg_max

                agg_avg = event.get("cpu_aggregate_busy_avg")
                event_data["cpu_aggregate_busy_avg"] = agg_avg if agg_avg

                # Remove nil values
                event_data.delete_if { |_k, v| v.nil? }

                payload = {
                    "sourcetype" => "aviatrix:gateway:system",
                    "source" => "avx-gw-sys-stats",
                    "host" => event.get("gateway"),
                    "time" => event.get("unix_time"),
                    "event" => event_data
                }

                event.set("[@metadata][sys_stats_hec_payload]", payload.to_json)
            '
        }
    }
}

# ============================================================================
# OUTPUT CONFIGURATION (dynatrace-metrics)
# ============================================================================

# Dynatrace Metrics Ingest API Output (MINT protocol)
# Sends gateway metrics as time-series data to Dynatrace
#
# Environment Variables:
#   DT_METRICS_URL     - Full Dynatrace metrics ingest URL
#                        e.g. https://abc12345.live.dynatrace.com/api/v2/metrics/ingest
#   DT_API_TOKEN       - Dynatrace API token with "metrics.ingest" scope
#   DT_METRIC_SOURCE   - Source dimension value (default: "aviatrix")
#   LOG_PROFILE        - Which log types to forward (default: all)
#                        - all: Forward all metrics
#                        - networking: gw_net_stats, gw_sys_stats
#
# MINT Format: metric.key,dim1="val1",dim2="val2" gauge,VALUE TIMESTAMP_MS
# See: https://docs.dynatrace.com/docs/dynatrace-api/environment-api/metric-v2/post-ingest-metrics

# Build MINT payload for gateway system stats
filter {
    if "gw_sys_stats" in [tags] {
        ruby {
            id => "dynatrace-build-sys-stats-mint"
            code => '
                gw = event.get("gateway") || "unknown"
                ali = event.get("alias") || gw
                src = ENV.fetch("DT_METRIC_SOURCE", "aviatrix")
                ts = (event.get("@timestamp").to_f * 1000).to_i

                # Escape dimension values for MINT protocol
                esc = lambda { |v| v.to_s.gsub("\\", "\\\\").gsub("\"", "\\\"") }

                dims = "gateway=\"#{esc.call(gw)}\",alias=\"#{esc.call(ali)}\",source=\"#{esc.call(src)}\""
                lines = []

                # CPU metrics
                cpu_idle = event.get("cpu_idle")
                if cpu_idle
                    idle_f = cpu_idle.to_f
                    lines << "aviatrix.gateway.cpu.idle,#{dims} gauge,#{idle_f} #{ts}"
                    lines << "aviatrix.gateway.cpu.usage,#{dims} gauge,#{(100 - idle_f).round(2)} #{ts}"
                end

                # Memory metrics (raw values are kB, convert to bytes)
                mem_avail = event.get("memory_available")
                mem_total = event.get("memory_total")
                mem_free  = event.get("memory_free")
                if mem_avail && mem_total
                    avail_f = mem_avail.to_f
                    total_f = mem_total.to_f
                    free_f  = mem_free.to_f

                    lines << "aviatrix.gateway.memory.avail,#{dims} gauge,#{(avail_f * 1024).to_i} #{ts}"
                    lines << "aviatrix.gateway.memory.total,#{dims} gauge,#{(total_f * 1024).to_i} #{ts}"
                    lines << "aviatrix.gateway.memory.free,#{dims} gauge,#{(free_f * 1024).to_i} #{ts}"
                    lines << "aviatrix.gateway.memory.used,#{dims} gauge,#{((total_f - avail_f) * 1024).to_i} #{ts}"
                    if total_f > 0
                        lines << "aviatrix.gateway.memory.usage,#{dims} gauge,#{((1.0 - avail_f / total_f) * 100).round(2)} #{ts}"
                    end
                end

                # Disk metrics (raw values are kB, convert to bytes)
                disk_total = event.get("disk_total")
                disk_free  = event.get("disk_free")
                if disk_total && disk_free
                    dt_f = disk_total.to_f
                    df_f = disk_free.to_f

                    lines << "aviatrix.gateway.disk.avail,#{dims} gauge,#{(df_f * 1024).to_i} #{ts}"
                    lines << "aviatrix.gateway.disk.total,#{dims} gauge,#{(dt_f * 1024).to_i} #{ts}"
                    lines << "aviatrix.gateway.disk.used,#{dims} gauge,#{((dt_f - df_f) * 1024).to_i} #{ts}"
                    if dt_f > 0
                        lines << "aviatrix.gateway.disk.used.percent,#{dims} gauge,#{((1.0 - df_f / dt_f) * 100).round(2)} #{ts}"
                    end
                end

                # Per-core CPU metrics (from filter 17 cpu_cores_parsed)
                cpu_cores_parsed = event.get("cpu_cores_parsed")
                if cpu_cores_parsed.is_a?(Array)
                    cpu_cores_parsed.each do |core|
                        core_name = core["name"]
                        next unless core_name
                        core_dim = core_name == "-1" ? "aggregate" : core_name
                        cdims = "#{dims},core=\"#{esc.call(core_dim)}\""

                        if core["busy_avg"]
                            busy = core["busy_avg"].to_f
                            lines << "aviatrix.gateway.cpu.idle,#{cdims} gauge,#{(100 - busy).round(2)} #{ts}"
                            lines << "aviatrix.gateway.cpu.usage,#{cdims} gauge,#{busy} #{ts}"
                        end
                    end
                end

                event.set("[@metadata][dynatrace_mint_payload]", lines.join("\n")) unless lines.empty?
            '
        }
    }
}

# Build MINT payload for gateway network stats
filter {
    if "gw_net_stats" in [tags] {
        ruby {
            id => "dynatrace-build-net-stats-mint"
            code => '
                gw = event.get("gateway") || "unknown"
                ali = event.get("alias") || gw
                src = ENV.fetch("DT_METRIC_SOURCE", "aviatrix")
                ts = (event.get("@timestamp").to_f * 1000).to_i
                iface = event.get("interface") || "unknown"
                pub_ip = event.get("public_ip")
                priv_ip = event.get("private_ip") || "unknown"

                esc = lambda { |v| v.to_s.gsub("\\", "\\\\").gsub("\"", "\\\"") }

                dims = "gateway=\"#{esc.call(gw)}\",alias=\"#{esc.call(ali)}\",source=\"#{esc.call(src)}\""
                dims += ",interface=\"#{esc.call(iface)}\""
                dims += ",public_ip=\"#{esc.call(pub_ip)}\"" if pub_ip && pub_ip.to_s != ""
                dims += ",private_ip=\"#{esc.call(priv_ip)}\""

                # Parse human-readable byte strings to numeric bytes
                # Handles: "54.07Kb" "2.49GB" "510.30MB" "3.73" "13.45KB"
                parse_to_bytes = lambda do |val|
                    return nil unless val
                    s = val.to_s.strip
                    return nil if s.empty?
                    m = s.match(/\A([\d.]+)\s*(KB|MB|GB|TB|Kb|Mb|Gb|B)?\z/i)
                    return s.to_f unless m
                    num = m[1].to_f
                    unit = m[2]
                    return num unless unit
                    case unit
                    when "B"
                        num
                    when "Kb", "KB", "kb"
                        num * 1024
                    when "Mb", "MB", "mb"
                        num * 1024 * 1024
                    when "Gb", "GB", "gb"
                        num * 1024 * 1024 * 1024
                    when "Tb", "TB", "tb"
                        num * 1024 * 1024 * 1024 * 1024
                    else
                        num
                    end
                end

                lines = []

                # Rate metrics (from raw strings preserved in [@metadata] by filter 94)
                rx_rate = parse_to_bytes.call(event.get("[@metadata][raw_total_rx_rate]"))
                tx_rate = parse_to_bytes.call(event.get("[@metadata][raw_total_tx_rate]"))
                rxtx_rate = parse_to_bytes.call(event.get("[@metadata][raw_total_rx_tx_rate]"))

                lines << "aviatrix.gateway.net.bytes_rx,#{dims} gauge,#{rx_rate.round(2)} #{ts}" if rx_rate
                lines << "aviatrix.gateway.net.bytes_tx,#{dims} gauge,#{tx_rate.round(2)} #{ts}" if tx_rate
                lines << "aviatrix.gateway.net.bytes_total_rate,#{dims} gauge,#{rxtx_rate.round(2)} #{ts}" if rxtx_rate

                # Cumulative metrics (from raw strings preserved in [@metadata] by filter 94)
                rx_cum = parse_to_bytes.call(event.get("[@metadata][raw_total_rx_cum]"))
                tx_cum = parse_to_bytes.call(event.get("[@metadata][raw_total_tx_cum]"))
                rxtx_cum = parse_to_bytes.call(event.get("[@metadata][raw_total_rx_tx_cum]"))

                lines << "aviatrix.gateway.net.rx_cumulative,#{dims} gauge,#{rx_cum.round(2)} #{ts}" if rx_cum
                lines << "aviatrix.gateway.net.tx_cumulative,#{dims} gauge,#{tx_cum.round(2)} #{ts}" if tx_cum
                lines << "aviatrix.gateway.net.rx_tx_cumulative,#{dims} gauge,#{rxtx_cum.round(2)} #{ts}" if rxtx_cum

                # Conntrack gauge metrics
                ct_count = event.get("conntrack_count")
                lines << "aviatrix.gateway.net.conntrack.count,#{dims} gauge,#{ct_count.to_i} #{ts}" if ct_count && ct_count.to_s != ""

                ct_avail = event.get("conntrack_allowance_available")
                lines << "aviatrix.gateway.net.conntrack.avail,#{dims} gauge,#{ct_avail.to_i} #{ts}" if ct_avail && ct_avail.to_s != ""

                ct_usage = event.get("conntrack_usage_rate")
                if ct_usage && ct_usage.to_s != ""
                    lines << "aviatrix.gateway.net.conntrack.usage,#{dims} gauge,#{(ct_usage.to_f * 100).round(2)} #{ts}"
                end

                # Limit-exceeded count metrics (delta counters)
                %w[conntrack_limit_exceeded bw_in_limit_exceeded bw_out_limit_exceeded pps_limit_exceeded linklocal_limit_exceeded].each do |field|
                    val = event.get(field)
                    if val && val.to_s != ""
                        lines << "aviatrix.gateway.net.#{field},#{dims} count,delta=#{val.to_i} #{ts}"
                    end
                end

                event.set("[@metadata][dynatrace_mint_payload]", lines.join("\n")) unless lines.empty?
            '
        }
    }
}

output {
    # Send metrics to Dynatrace Metrics Ingest API v2
    if [@metadata][dynatrace_mint_payload] {
        if ("gw_net_stats" in [tags] or "gw_sys_stats" in [tags]) and ("${LOG_PROFILE:all}" == "all" or "${LOG_PROFILE:all}" == "networking") {
            http {
                id => "dynatrace-metrics"
                http_method => "post"
                url => "${DT_METRICS_URL}"
                headers => {
                    "Authorization" => "Api-Token ${DT_API_TOKEN}"
                }
                content_type => "text/plain; charset=utf-8"
                format => "message"
                message => "%{[@metadata][dynatrace_mint_payload]}"
                pool_max => 10
                pool_max_per_route => 5
                socket_timeout => 30
            }
        }
    }
}
